{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: When running this notebook on SageMaker Studio, you should make sure the 'SageMaker JumpStart Tensorflow 1.0' image/kernel is used. You can run run all cells at once or step through the notebook.\n",
    "# Policy Training\n",
    "\n",
    "This notebook outlines the steps involved in building and deploying a Battlesnake model using Ray RLlib and TensorFlow on Amazon SageMaker.\n",
    "\n",
    "Library versions currently in use:  TensorFlow 2.1, Ray RLlib 0.8.2\n",
    "\n",
    "The model is first trained using multi-agent PPO, and then deployed to a managed _TensorFlow Serving_ SageMaker endpoint that can be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.rl import RLEstimator, RLToolkit\n",
    "import boto3\n",
    "import botocore\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../stack_outputs.json\") as f:\n",
    "    info = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise sagemaker\n",
    "We need to define several parameters prior to running the training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket path: s3://sagemaker-soln-bs-rbcsnake-bucket/\n"
     ]
    }
   ],
   "source": [
    "sm_session = sagemaker.session.Session()\n",
    "s3_bucket = info[\"S3Bucket\"]\n",
    "\n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::018864217387:role/sagemaker-soln-bs-us-west-2-nb-role\n"
     ]
    }
   ],
   "source": [
    "job_name_prefix = info[\"SolutionPrefix\"]+'-job-rllib'\n",
    "\n",
    "role = info[\"SageMakerIamRoleArn\"]\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change local_mode to True if you want to do local training within this Notebook instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    instance_type = 'local'\n",
    "else:\n",
    "    instance_type = info[\"SagemakerTrainingInstanceType\"]\n",
    "    \n",
    "# If training locally, do some Docker housekeeping..\n",
    "if local_mode:\n",
    "    !/bin/bash ./common/setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sm_session.boto_region_name\n",
    "device = \"cpu\"\n",
    "image_name = '462105765813.dkr.ecr.{region}.amazonaws.com/sagemaker-rl-ray-container:ray-0.8.2-tf-{device}-py36'.format(region=region, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-25 02:29:54 Starting - Starting the training job...\n",
      "2022-02-25 02:29:56 Starting - Launching requested ML instancesProfilerReport-1645756193: InProgress\n",
      ".........\n",
      "2022-02-25 02:31:52 Starting - Preparing the instances for training......\n",
      "2022-02-25 02:32:53 Downloading - Downloading input data\n",
      "2022-02-25 02:32:53 Training - Downloading the training image......\n",
      "2022-02-25 02:33:53 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-02-25 02:33:44,450 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2022-02-25 02:33:44,457 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-02-25 02:33:44,627 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting array2gif\n",
      "  Downloading array2gif-1.0.4-py3-none-any.whl (7.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting mxboard\n",
      "  Downloading mxboard-0.1.0-py3-none-any.whl (47 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from array2gif->-r requirements.txt (line 1)) (1.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from mxboard->-r requirements.txt (line 2)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from mxboard->-r requirements.txt (line 2)) (3.11.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from mxboard->-r requirements.txt (line 2)) (7.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym->-r requirements.txt (line 3)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->-r requirements.txt (line 3)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->-r requirements.txt (line 3)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.0.0->mxboard->-r requirements.txt (line 2)) (46.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->-r requirements.txt (line 3)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->-r requirements.txt (line 3)) (2020.4.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->-r requirements.txt (line 3)) (1.25.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->-r requirements.txt (line 3)) (2.8)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: battlesnake-gym\n",
      "  Building wheel for battlesnake-gym (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for battlesnake-gym (setup.py): finished with status 'done'\n",
      "  Created wheel for battlesnake-gym: filename=battlesnake_gym-0.1.dev0-py3-none-any.whl size=16942 sha256=06c2249a7bf8da15d5a75d854a7d1412cb9f7c888700995c355600b64860446f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-h686_sbw/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca\u001b[0m\n",
      "\u001b[34mSuccessfully built battlesnake-gym\u001b[0m\n",
      "\u001b[34mInstalling collected packages: array2gif, mxboard, battlesnake-gym\u001b[0m\n",
      "\u001b[34mSuccessfully installed array2gif-1.0.4 battlesnake-gym-0.1.dev0 mxboard-0.1.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.2; however, version 21.3.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2022-02-25 02:33:46,991 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-02-25 02:33:47,006 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-02-25 02:33:47,020 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-02-25 02:33:47,031 sagemaker-containers INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_estimator\": \"RLEstimator\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"num_agents\": 5,\n",
      "        \"use_heuristics_action_masks\": false,\n",
      "        \"additional_configs\": {\n",
      "            \"lambda\": 0.9,\n",
      "            \"gamma\": 0.999,\n",
      "            \"kl_coeff\": 0.2,\n",
      "            \"clip_rewards\": true,\n",
      "            \"vf_clip_param\": 175.0,\n",
      "            \"train_batch_size\": 9216,\n",
      "            \"sample_batch_size\": 96,\n",
      "            \"sgd_minibatch_size\": 256,\n",
      "            \"num_sgd_iter\": 3,\n",
      "            \"lr\": 0.0005\n",
      "        },\n",
      "        \"map_size\": 11,\n",
      "        \"num_iters\": 10,\n",
      "        \"iterate_map_size\": false,\n",
      "        \"algorithm\": \"PPO\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-soln-bs-job-rllib-2022-02-25-02-29-53-333\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-soln-bs-rbcsnake-bucket/sagemaker-soln-bs-job-rllib-2022-02-25-02-29-53-333/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-mabs\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-mabs.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"additional_configs\":{\"clip_rewards\":true,\"gamma\":0.999,\"kl_coeff\":0.2,\"lambda\":0.9,\"lr\":0.0005,\"num_sgd_iter\":3,\"sample_batch_size\":96,\"sgd_minibatch_size\":256,\"train_batch_size\":9216,\"vf_clip_param\":175.0},\"algorithm\":\"PPO\",\"iterate_map_size\":false,\"map_size\":11,\"num_agents\":5,\"num_iters\":10,\"use_heuristics_action_masks\":false}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-mabs.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_estimator\":\"RLEstimator\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-mabs\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-soln-bs-rbcsnake-bucket/sagemaker-soln-bs-job-rllib-2022-02-25-02-29-53-333/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_estimator\":\"RLEstimator\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"additional_configs\":{\"clip_rewards\":true,\"gamma\":0.999,\"kl_coeff\":0.2,\"lambda\":0.9,\"lr\":0.0005,\"num_sgd_iter\":3,\"sample_batch_size\":96,\"sgd_minibatch_size\":256,\"train_batch_size\":9216,\"vf_clip_param\":175.0},\"algorithm\":\"PPO\",\"iterate_map_size\":false,\"map_size\":11,\"num_agents\":5,\"num_iters\":10,\"use_heuristics_action_masks\":false},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-soln-bs-job-rllib-2022-02-25-02-29-53-333\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-soln-bs-rbcsnake-bucket/sagemaker-soln-bs-job-rllib-2022-02-25-02-29-53-333/source/sourcedir.tar.gz\",\"module_name\":\"train-mabs\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-mabs.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--additional_configs\",\"clip_rewards=True,gamma=0.999,kl_coeff=0.2,lambda=0.9,lr=0.0005,num_sgd_iter=3,sample_batch_size=96,sgd_minibatch_size=256,train_batch_size=9216,vf_clip_param=175.0\",\"--algorithm\",\"PPO\",\"--iterate_map_size\",\"False\",\"--map_size\",\"11\",\"--num_agents\",\"5\",\"--num_iters\",\"10\",\"--use_heuristics_action_masks\",\"False\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_AGENTS=5\u001b[0m\n",
      "\u001b[34mSM_HP_USE_HEURISTICS_ACTION_MASKS=false\u001b[0m\n",
      "\u001b[34mSM_HP_ADDITIONAL_CONFIGS={\"clip_rewards\":true,\"gamma\":0.999,\"kl_coeff\":0.2,\"lambda\":0.9,\"lr\":0.0005,\"num_sgd_iter\":3,\"sample_batch_size\":96,\"sgd_minibatch_size\":256,\"train_batch_size\":9216,\"vf_clip_param\":175.0}\u001b[0m\n",
      "\u001b[34mSM_HP_MAP_SIZE=11\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_ITERS=10\u001b[0m\n",
      "\u001b[34mSM_HP_ITERATE_MAP_SIZE=false\u001b[0m\n",
      "\u001b[34mSM_HP_ALGORITHM=PPO\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 -m train-mabs --additional_configs clip_rewards=True,gamma=0.999,kl_coeff=0.2,lambda=0.9,lr=0.0005,num_sgd_iter=3,sample_batch_size=96,sgd_minibatch_size=256,train_batch_size=9216,vf_clip_param=175.0 --algorithm PPO --iterate_map_size False --map_size 11 --num_agents 5 --num_iters 10 --use_heuristics_action_masks False\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2022-02-25 02:33:50,179#011INFO resource_spec.py:212 -- Starting Ray with 6.69 GiB memory available for workers and up to 3.36 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\u001b[0m\n",
      "\u001b[34m2022-02-25 02:33:50,597#011INFO services.py:1078 -- View the Ray dashboard at #033[1m#033[32mlocalhost:8265#033[39m#033[22m\u001b[0m\n",
      "\u001b[34mNo checkpoint path specified. Training from scratch.\u001b[0m\n",
      "\u001b[34mImportant! Ray with version <=7.2 may report \"Did not find checkpoint file\" even if the experiment is actually restored successfully. If restoration is expected, please check \"training_iteration\" in the experiment info to confirm.\u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 2.3/15.4 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+-------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc   |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+-------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_5e0c2aea | RUNNING  |       |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+-------+\u001b[0m\n",
      "\u001b[34m2022-02-25 02:33:52,385#011WARNING worker.py:1058 -- The dashboard on node ip-10-0-208-182.us-west-2.compute.internal failed with the following error:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1062, in create_server\n",
      "    sock.bind(sa)\u001b[0m\n",
      "\u001b[34mOSError: [Errno 99] Cannot assign requested address\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/dashboard/dashboard.py\", line 920, in <module>\n",
      "    dashboard.run()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/dashboard/dashboard.py\", line 368, in run\n",
      "    aiohttp.web.run_app(self.app, host=self.host, port=self.port)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web.py\", line 433, in run_app\n",
      "    reuse_port=reuse_port))\n",
      "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 484, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web.py\", line 359, in _run_app\n",
      "    await site.start()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web_runner.py\", line 104, in start\n",
      "    reuse_port=self._reuse_port)\n",
      "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1066, in create_server\n",
      "    % (sa, err.strerror.lower()))\u001b[0m\n",
      "\u001b[34mOSError: [Errno 99] error while attempting to bind on address ('::1', 8265, 0, 0): cannot assign requested address\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=117)#033[0m 2022-02-25 02:33:54,198#011INFO trainer.py:420 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=117)#033[0m 2022-02-25 02:33:54,200#011INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=117)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=117)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=117)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=117)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=117)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=117)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=117)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=117)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=117)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=117)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m   obj = yaml.load(type_)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=117)#033[0m 2022-02-25 02:34:33,590#011INFO trainable.py:178 -- _setup took 39.392 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_5e0c2aea:\n",
      "  best_snake_episode_len_max: 15\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 10\n",
      "    Forbidden_move_mean: 3.825323475046211\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 3\n",
      "    Killed_another_snake_mean: 0.10905730129390019\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 4\n",
      "    Snake_hit_body_mean: 0.24306839186691312\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 7\n",
      "    Snake_hit_wall_mean: 1.3969500924214417\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.3257855822550832\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 14\n",
      "    policy0_max_len_mean: 2.1950092421441774\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 12\n",
      "    policy1_max_len_mean: 2.1829944547134934\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 11\n",
      "    policy2_max_len_mean: 2.1732902033271717\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 15\n",
      "    policy3_max_len_mean: 2.21996303142329\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 13\n",
      "    policy4_max_len_mean: 2.2102587800369684\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-02-25_02-36-04\n",
      "  done: false\n",
      "  episode_len_max: 15\n",
      "  episode_len_mean: 4.25554528650647\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 37.0\n",
      "  episode_reward_mean: 10.981515711645102\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2164\n",
      "  episodes_total: 2164\n",
      "  experiment_id: 3217e5ecf36d427a81fa4f34f76a710e\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-208-182.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 20428.05\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3799711465835571\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0070586372166872025\n",
      "        policy_loss: -0.015434210188686848\n",
      "        total_loss: 1.511623740196228\n",
      "        vf_explained_var: 0.1819339543581009\n",
      "        vf_loss: 1.5256459712982178\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3756262063980103\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00823091808706522\n",
      "        policy_loss: -0.018285490572452545\n",
      "        total_loss: 1.532055139541626\n",
      "        vf_explained_var: 0.18241621553897858\n",
      "        vf_loss: 1.5486946105957031\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.375399112701416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006874500308185816\n",
      "        policy_loss: -0.016286209225654602\n",
      "        total_loss: 1.459864854812622\n",
      "        vf_explained_var: 0.18901315331459045\n",
      "        vf_loss: 1.474776029586792\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3720786571502686\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010484538972377777\n",
      "        policy_loss: -0.020860059186816216\n",
      "        total_loss: 1.5239007472991943\n",
      "        vf_explained_var: 0.18025028705596924\n",
      "        vf_loss: 1.5426641702651978\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3586772680282593\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012403714470565319\n",
      "        policy_loss: -0.0196414515376091\n",
      "        total_loss: 1.506022572517395\n",
      "        vf_explained_var: 0.1874564290046692\n",
      "        vf_loss: 1.5231832265853882\n",
      "    load_time_ms: 2827.121\n",
      "    num_steps_sampled: 9216\n",
      "    num_steps_trained: 9216\n",
      "    sample_time_ms: 39592.34\n",
      "    update_time_ms: 23433.53\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.208.182\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.6424\n",
      "    ram_util_percent: 34.412800000000004\n",
      "  pid: 117\n",
      "  policy_reward_max:\n",
      "    policy_0: 14.0\n",
      "    policy_1: 12.0\n",
      "    policy_2: 11.0\n",
      "    policy_3: 15.0\n",
      "    policy_4: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 2.1950092421441774\n",
      "    policy_1: 2.1829944547134934\n",
      "    policy_2: 2.1732902033271717\n",
      "    policy_3: 2.21996303142329\n",
      "    policy_4: 2.2102587800369684\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.2296627511421105\n",
      "    mean_inference_ms: 7.198160937330125\n",
      "    mean_processing_ms: 2.744093848552278\n",
      "  time_since_restore: 88.28360223770142\n",
      "  time_this_iter_s: 88.28360223770142\n",
      "  time_total_s: 88.28360223770142\n",
      "  timestamp: 1645756564\n",
      "  timesteps_since_restore: 9216\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 9216\n",
      "  training_iteration: 1\n",
      "  trial_id: 5e0c2aea\n",
      "  worst_snake_episode_len_max: 11\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.4 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |   ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_5e0c2aea | RUNNING  | 10.0.208.182:117 |  10.9815 |          88.2836 | 9216 |      1 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+------+--------+\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_5e0c2aea:\n",
      "  best_snake_episode_len_max: 14\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 10\n",
      "    Forbidden_move_mean: 3.878787878787879\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 3\n",
      "    Killed_another_snake_mean: 0.11436950146627566\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 5\n",
      "    Snake_hit_body_mean: 0.25317693059628543\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 6\n",
      "    Snake_hit_wall_mean: 1.3176930596285434\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 6\n",
      "    Snake_was_eaten_mean: 0.27419354838709675\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 13\n",
      "    policy0_max_len_mean: 2.3147605083088956\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 11\n",
      "    policy1_max_len_mean: 2.3025415444770285\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 14\n",
      "    policy2_max_len_mean: 2.355816226783969\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 14\n",
      "    policy3_max_len_mean: 2.36950146627566\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 12\n",
      "    policy4_max_len_mean: 2.36950146627566\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-02-25_02-36-54\n",
      "  done: false\n",
      "  episode_len_max: 15\n",
      "  episode_len_mean: 4.504887585532747\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 38.0\n",
      "  episode_reward_mean: 11.712121212121213\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2046\n",
      "  episodes_total: 4210\n",
      "  experiment_id: 3217e5ecf36d427a81fa4f34f76a710e\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-208-182.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 15832.378\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3534847497940063\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01612992025911808\n",
      "        policy_loss: -0.0385943204164505\n",
      "        total_loss: 1.6404855251312256\n",
      "        vf_explained_var: 0.3031264543533325\n",
      "        vf_loss: 1.6758538484573364\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3408774137496948\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01855674386024475\n",
      "        policy_loss: -0.04241649806499481\n",
      "        total_loss: 1.5065865516662598\n",
      "        vf_explained_var: 0.3228330612182617\n",
      "        vf_loss: 1.5452919006347656\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3537468910217285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015978718176484108\n",
      "        policy_loss: -0.03549330309033394\n",
      "        total_loss: 1.5465384721755981\n",
      "        vf_explained_var: 0.30648109316825867\n",
      "        vf_loss: 1.5788359642028809\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.340944528579712\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01989501714706421\n",
      "        policy_loss: -0.04441303387284279\n",
      "        total_loss: 1.6988778114318848\n",
      "        vf_explained_var: 0.29126569628715515\n",
      "        vf_loss: 1.739311695098877\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3024629354476929\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01911761984229088\n",
      "        policy_loss: -0.038602739572525024\n",
      "        total_loss: 1.6255455017089844\n",
      "        vf_explained_var: 0.31677064299583435\n",
      "        vf_loss: 1.6603245735168457\n",
      "    load_time_ms: 1766.028\n",
      "    num_steps_sampled: 18432\n",
      "    num_steps_trained: 18432\n",
      "    sample_time_ms: 38601.624\n",
      "    update_time_ms: 11730.405\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.0.208.182\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.73692307692309\n",
      "    ram_util_percent: 43.50153846153846\n",
      "  pid: 117\n",
      "  policy_reward_max:\n",
      "    policy_0: 13.0\n",
      "    policy_1: 11.0\n",
      "    policy_2: 14.0\n",
      "    policy_3: 14.0\n",
      "    policy_4: 12.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 2.3147605083088956\n",
      "    policy_1: 2.3025415444770285\n",
      "    policy_2: 2.355816226783969\n",
      "    policy_3: 2.36950146627566\n",
      "    policy_4: 2.36950146627566\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.2552016892468933\n",
      "    mean_inference_ms: 7.018493186632347\n",
      "    mean_processing_ms: 2.735766242331397\n",
      "  time_since_restore: 138.12579989433289\n",
      "  time_this_iter_s: 49.84219765663147\n",
      "  time_total_s: 138.12579989433289\n",
      "  timestamp: 1645756614\n",
      "  timesteps_since_restore: 18432\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 18432\n",
      "  training_iteration: 2\n",
      "  trial_id: 5e0c2aea\n",
      "  worst_snake_episode_len_max: 11\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.4 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_5e0c2aea | RUNNING  | 10.0.208.182:117 |  11.7121 |          138.126 | 18432 |      2 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_5e0c2aea:\n",
      "  best_snake_episode_len_max: 15\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 10\n",
      "    Forbidden_move_mean: 4.035846072746442\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 3\n",
      "    Killed_another_snake_mean: 0.13547706905640486\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 4\n",
      "    Snake_hit_body_mean: 0.2836056931997891\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 6\n",
      "    Snake_hit_wall_mean: 1.0542962572482868\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.29467580390089615\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 13\n",
      "    policy0_max_len_mean: 2.518186610437533\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 13\n",
      "    policy1_max_len_mean: 2.6389035318924616\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 15\n",
      "    policy2_max_len_mean: 2.4802319451765946\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 13\n",
      "    policy3_max_len_mean: 2.5861887190300474\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 15\n",
      "    policy4_max_len_mean: 2.6125461254612548\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-02-25_02-37-44\n",
      "  done: false\n",
      "  episode_len_max: 15\n",
      "  episode_len_mean: 4.85872430152873\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 41.0\n",
      "  episode_reward_mean: 12.836056931997891\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 1897\n",
      "  episodes_total: 6107\n",
      "  experiment_id: 3217e5ecf36d427a81fa4f34f76a710e\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-208-182.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 14319.144\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3070706129074097\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020238526165485382\n",
      "        policy_loss: -0.04993142560124397\n",
      "        total_loss: 1.8706245422363281\n",
      "        vf_explained_var: 0.33664679527282715\n",
      "        vf_loss: 1.9165085554122925\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2887506484985352\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023430636152625084\n",
      "        policy_loss: -0.05146288499236107\n",
      "        total_loss: 2.0269689559936523\n",
      "        vf_explained_var: 0.32431113719940186\n",
      "        vf_loss: 2.0737454891204834\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.302417278289795\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02187434583902359\n",
      "        policy_loss: -0.048919927328825\n",
      "        total_loss: 1.9385381937026978\n",
      "        vf_explained_var: 0.32089146971702576\n",
      "        vf_loss: 1.9830832481384277\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2699940204620361\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02423211745917797\n",
      "        policy_loss: -0.057077452540397644\n",
      "        total_loss: 1.909706711769104\n",
      "        vf_explained_var: 0.33299940824508667\n",
      "        vf_loss: 1.9619380235671997\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2316631078720093\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025384416803717613\n",
      "        policy_loss: -0.05619291588664055\n",
      "        total_loss: 2.107146978378296\n",
      "        vf_explained_var: 0.3448355197906494\n",
      "        vf_loss: 2.1582632064819336\n",
      "    load_time_ms: 1410.368\n",
      "    num_steps_sampled: 27648\n",
      "    num_steps_trained: 27648\n",
      "    sample_time_ms: 38170.261\n",
      "    update_time_ms: 7829.231\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.0.208.182\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.57343750000001\n",
      "    ram_util_percent: 43.584374999999994\n",
      "  pid: 117\n",
      "  policy_reward_max:\n",
      "    policy_0: 13.0\n",
      "    policy_1: 13.0\n",
      "    policy_2: 15.0\n",
      "    policy_3: 13.0\n",
      "    policy_4: 15.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 2.518186610437533\n",
      "    policy_1: 2.6389035318924616\n",
      "    policy_2: 2.4802319451765946\n",
      "    policy_3: 2.5861887190300474\n",
      "    policy_4: 2.6125461254612548\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.2579861742578298\n",
      "    mean_inference_ms: 6.966556345782445\n",
      "    mean_processing_ms: 2.7065692214321637\n",
      "  time_since_restore: 187.52987217903137\n",
      "  time_this_iter_s: 49.404072284698486\n",
      "  time_total_s: 187.52987217903137\n",
      "  timestamp: 1645756664\n",
      "  timesteps_since_restore: 27648\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 27648\n",
      "  training_iteration: 3\n",
      "  trial_id: 5e0c2aea\n",
      "  worst_snake_episode_len_max: 13\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.4 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_5e0c2aea | RUNNING  | 10.0.208.182:117 |  12.8361 |           187.53 | 27648 |      3 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_5e0c2aea:\n",
      "  best_snake_episode_len_max: 18\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 10\n",
      "    Forbidden_move_mean: 4.10377358490566\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 3\n",
      "    Killed_another_snake_mean: 0.14858490566037735\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 4\n",
      "    Snake_hit_body_mean: 0.32547169811320753\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 5\n",
      "    Snake_hit_wall_mean: 0.7494103773584906\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.40153301886792453\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 14\n",
      "    policy0_max_len_mean: 2.73938679245283\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 16\n",
      "    policy1_max_len_mean: 2.8997641509433962\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 18\n",
      "    policy2_max_len_mean: 2.891509433962264\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 17\n",
      "    policy3_max_len_mean: 3.0306603773584904\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 18\n",
      "    policy4_max_len_mean: 3.0247641509433962\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-02-25_02-38-32\n",
      "  done: false\n",
      "  episode_len_max: 19\n",
      "  episode_len_mean: 5.435141509433962\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 47.0\n",
      "  episode_reward_mean: 14.586084905660377\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 1696\n",
      "  episodes_total: 7803\n",
      "  experiment_id: 3217e5ecf36d427a81fa4f34f76a710e\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-208-182.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 13516.236\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2573426961898804\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023205123841762543\n",
      "        policy_loss: -0.05567388981580734\n",
      "        total_loss: 2.072171926498413\n",
      "        vf_explained_var: 0.3654915690422058\n",
      "        vf_loss: 2.120884418487549\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2147940397262573\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02537275291979313\n",
      "        policy_loss: -0.05745691806077957\n",
      "        total_loss: 2.313703775405884\n",
      "        vf_explained_var: 0.36275991797447205\n",
      "        vf_loss: 2.363548517227173\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2312891483306885\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02288317307829857\n",
      "        policy_loss: -0.05304373800754547\n",
      "        total_loss: 2.344419479370117\n",
      "        vf_explained_var: 0.3514936864376068\n",
      "        vf_loss: 2.3905982971191406\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2103240489959717\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022693416103720665\n",
      "        policy_loss: -0.05627420172095299\n",
      "        total_loss: 2.4779324531555176\n",
      "        vf_explained_var: 0.3485909104347229\n",
      "        vf_loss: 2.5273983478546143\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1621013879776\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022254357114434242\n",
      "        policy_loss: -0.05479288473725319\n",
      "        total_loss: 2.6466856002807617\n",
      "        vf_explained_var: 0.3746274709701538\n",
      "        vf_loss: 2.6948022842407227\n",
      "    load_time_ms: 1217.968\n",
      "    num_steps_sampled: 36864\n",
      "    num_steps_trained: 36864\n",
      "    sample_time_ms: 37729.58\n",
      "    update_time_ms: 5877.588\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.0.208.182\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.22857142857144\n",
      "    ram_util_percent: 43.65555555555557\n",
      "  pid: 117\n",
      "  policy_reward_max:\n",
      "    policy_0: 14.0\n",
      "    policy_1: 16.0\n",
      "    policy_2: 18.0\n",
      "    policy_3: 17.0\n",
      "    policy_4: 18.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 2.73938679245283\n",
      "    policy_1: 2.8997641509433962\n",
      "    policy_2: 2.891509433962264\n",
      "    policy_3: 3.0306603773584904\n",
      "    policy_4: 3.0247641509433962\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.2624650846391186\n",
      "    mean_inference_ms: 6.9123791107963735\n",
      "    mean_processing_ms: 2.6501770556714312\n",
      "  time_since_restore: 235.78404474258423\n",
      "  time_this_iter_s: 48.254172563552856\n",
      "  time_total_s: 235.78404474258423\n",
      "  timestamp: 1645756712\n",
      "  timesteps_since_restore: 36864\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 36864\n",
      "  training_iteration: 4\n",
      "  trial_id: 5e0c2aea\n",
      "  worst_snake_episode_len_max: 14\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.4 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_5e0c2aea | RUNNING  | 10.0.208.182:117 |  14.5861 |          235.784 | 36864 |      4 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_5e0c2aea:\n",
      "  best_snake_episode_len_max: 22\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 9\n",
      "    Forbidden_move_mean: 4.139118457300276\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 3\n",
      "    Killed_another_snake_mean: 0.19765840220385675\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 6\n",
      "    Snake_hit_body_mean: 0.4380165289256198\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 4\n",
      "    Snake_hit_wall_mean: 0.46763085399449034\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.4724517906336088\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 22\n",
      "    policy0_max_len_mean: 3.319559228650138\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 22\n",
      "    policy1_max_len_mean: 3.4056473829201104\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 22\n",
      "    policy2_max_len_mean: 3.462121212121212\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 21\n",
      "    policy3_max_len_mean: 3.451101928374656\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 17\n",
      "    policy4_max_len_mean: 3.6508264462809916\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-02-25_02-39-21\n",
      "  done: false\n",
      "  episode_len_max: 22\n",
      "  episode_len_mean: 6.339531680440771\n",
      "  episode_len_min: 2\n",
      "  episode_reward_max: 56.0\n",
      "  episode_reward_mean: 17.289256198347108\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 1452\n",
      "  episodes_total: 9255\n",
      "  experiment_id: 3217e5ecf36d427a81fa4f34f76a710e\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-208-182.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 13092.769\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1994047164916992\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02076241932809353\n",
      "        policy_loss: -0.05095488950610161\n",
      "        total_loss: 2.7701416015625\n",
      "        vf_explained_var: 0.385154128074646\n",
      "        vf_loss: 2.811753034591675\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1677619218826294\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019437015056610107\n",
      "        policy_loss: -0.05341574177145958\n",
      "        total_loss: 3.049325942993164\n",
      "        vf_explained_var: 0.3923076391220093\n",
      "        vf_loss: 3.0939950942993164\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1526209115982056\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019736412912607193\n",
      "        policy_loss: -0.04980778694152832\n",
      "        total_loss: 2.900367498397827\n",
      "        vf_explained_var: 0.38144969940185547\n",
      "        vf_loss: 2.941293954849243\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1587684154510498\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0194653682410717\n",
      "        policy_loss: -0.054874882102012634\n",
      "        total_loss: 2.9665822982788086\n",
      "        vf_explained_var: 0.389668345451355\n",
      "        vf_loss: 3.012697696685791\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0848422050476074\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021746523678302765\n",
      "        policy_loss: -0.05526221916079521\n",
      "        total_loss: 3.210280179977417\n",
      "        vf_explained_var: 0.41348373889923096\n",
      "        vf_loss: 3.255756378173828\n",
      "    load_time_ms: 1109.73\n",
      "    num_steps_sampled: 46080\n",
      "    num_steps_trained: 46080\n",
      "    sample_time_ms: 37448.705\n",
      "    update_time_ms: 4707.138\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.0.208.182\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.54920634920637\n",
      "    ram_util_percent: 43.70476190476193\n",
      "  pid: 117\n",
      "  policy_reward_max:\n",
      "    policy_0: 22.0\n",
      "    policy_1: 22.0\n",
      "    policy_2: 22.0\n",
      "    policy_3: 21.0\n",
      "    policy_4: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 3.319559228650138\n",
      "    policy_1: 3.4056473829201104\n",
      "    policy_2: 3.462121212121212\n",
      "    policy_3: 3.451101928374656\n",
      "    policy_4: 3.6508264462809916\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.2748803418527737\n",
      "    mean_inference_ms: 6.895229948246981\n",
      "    mean_processing_ms: 2.5865879532557297\n",
      "  time_since_restore: 284.2755982875824\n",
      "  time_this_iter_s: 48.49155354499817\n",
      "  time_total_s: 284.2755982875824\n",
      "  timestamp: 1645756761\n",
      "  timesteps_since_restore: 46080\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 46080\n",
      "  training_iteration: 5\n",
      "  trial_id: 5e0c2aea\n",
      "  worst_snake_episode_len_max: 17\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.4 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_5e0c2aea | RUNNING  | 10.0.208.182:117 |  17.2893 |          284.276 | 46080 |      5 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_5e0c2aea:\n",
      "  best_snake_episode_len_max: 23\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 8\n",
      "    Forbidden_move_mean: 4.053281853281853\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 3\n",
      "    Killed_another_snake_mean: 0.2277992277992278\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 5\n",
      "    Snake_hit_body_mean: 0.4957528957528958\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 5\n",
      "    Snake_hit_wall_mean: 0.3335907335907336\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.5590733590733591\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 22\n",
      "    policy0_max_len_mean: 3.6833976833976836\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 19\n",
      "    policy1_max_len_mean: 3.928957528957529\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 23\n",
      "    policy2_max_len_mean: 3.962162162162162\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 17\n",
      "    policy3_max_len_mean: 3.9436293436293437\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 18\n",
      "    policy4_max_len_mean: 4.240926640926641\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-02-25_02-40-09\n",
      "  done: false\n",
      "  episode_len_max: 23\n",
      "  episode_len_mean: 7.123552123552123\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 60.0\n",
      "  episode_reward_mean: 19.75907335907336\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 1295\n",
      "  episodes_total: 10550\n",
      "  experiment_id: 3217e5ecf36d427a81fa4f34f76a710e\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-208-182.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 12927.094\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.156481385231018\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015892647206783295\n",
      "        policy_loss: -0.049120742827653885\n",
      "        total_loss: 3.415422201156616\n",
      "        vf_explained_var: 0.40750935673713684\n",
      "        vf_loss: 3.4538156986236572\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1102051734924316\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017497487366199493\n",
      "        policy_loss: -0.049349911510944366\n",
      "        total_loss: 3.471459150314331\n",
      "        vf_explained_var: 0.42474624514579773\n",
      "        vf_loss: 3.512935161590576\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1049433946609497\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02035817876458168\n",
      "        policy_loss: -0.051830414682626724\n",
      "        total_loss: 3.4088568687438965\n",
      "        vf_explained_var: 0.4287029504776001\n",
      "        vf_loss: 3.451526165008545\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1024577617645264\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01892058737576008\n",
      "        policy_loss: -0.05422420799732208\n",
      "        total_loss: 3.5334315299987793\n",
      "        vf_explained_var: 0.4206022024154663\n",
      "        vf_loss: 3.57914137840271\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0385372638702393\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01531189028173685\n",
      "        policy_loss: -0.051400475203990936\n",
      "        total_loss: 3.5394129753112793\n",
      "        vf_explained_var: 0.4480598270893097\n",
      "        vf_loss: 3.580477714538574\n",
      "    load_time_ms: 1040.101\n",
      "    num_steps_sampled: 55296\n",
      "    num_steps_trained: 55296\n",
      "    sample_time_ms: 37107.34\n",
      "    update_time_ms: 3926.54\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.0.208.182\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.31746031746033\n",
      "    ram_util_percent: 43.83492063492065\n",
      "  pid: 117\n",
      "  policy_reward_max:\n",
      "    policy_0: 22.0\n",
      "    policy_1: 19.0\n",
      "    policy_2: 23.0\n",
      "    policy_3: 17.0\n",
      "    policy_4: 18.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 3.6833976833976836\n",
      "    policy_1: 3.928957528957529\n",
      "    policy_2: 3.962162162162162\n",
      "    policy_3: 3.9436293436293437\n",
      "    policy_4: 4.240926640926641\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.2766614418585545\n",
      "    mean_inference_ms: 6.860186455909295\n",
      "    mean_processing_ms: 2.5205127865710866\n",
      "  time_since_restore: 332.551673412323\n",
      "  time_this_iter_s: 48.2760751247406\n",
      "  time_total_s: 332.551673412323\n",
      "  timestamp: 1645756809\n",
      "  timesteps_since_restore: 55296\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 55296\n",
      "  training_iteration: 6\n",
      "  trial_id: 5e0c2aea\n",
      "  worst_snake_episode_len_max: 17\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.4 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_5e0c2aea | RUNNING  | 10.0.208.182:117 |  19.7591 |          332.552 | 55296 |      6 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_5e0c2aea:\n",
      "  best_snake_episode_len_max: 24\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 8\n",
      "    Forbidden_move_mean: 3.949615713065756\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 3\n",
      "    Killed_another_snake_mean: 0.28095644748078563\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 5\n",
      "    Snake_hit_body_mean: 0.5943637916310845\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 4\n",
      "    Snake_hit_wall_mean: 0.21007685738684884\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.6643894107600341\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 23\n",
      "    policy0_max_len_mean: 3.987190435525192\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 21\n",
      "    policy1_max_len_mean: 4.4534585824081985\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 24\n",
      "    policy2_max_len_mean: 4.5687446626814685\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 23\n",
      "    policy3_max_len_mean: 4.403928266438941\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 21\n",
      "    policy4_max_len_mean: 4.809564474807856\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-02-25_02-40-57\n",
      "  done: false\n",
      "  episode_len_max: 24\n",
      "  episode_len_mean: 7.865926558497011\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 79.0\n",
      "  episode_reward_mean: 22.222886421861656\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 1171\n",
      "  episodes_total: 11721\n",
      "  experiment_id: 3217e5ecf36d427a81fa4f34f76a710e\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-208-182.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 12679.768\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.116558313369751\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01607275754213333\n",
      "        policy_loss: -0.04830421507358551\n",
      "        total_loss: 3.8407957553863525\n",
      "        vf_explained_var: 0.41870105266571045\n",
      "        vf_loss: 3.878251075744629\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0530444383621216\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018776385113596916\n",
      "        policy_loss: -0.04898868501186371\n",
      "        total_loss: 3.856801748275757\n",
      "        vf_explained_var: 0.455694317817688\n",
      "        vf_loss: 3.8973405361175537\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0450060367584229\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016768794506788254\n",
      "        policy_loss: -0.0498177632689476\n",
      "        total_loss: 4.170750617980957\n",
      "        vf_explained_var: 0.42866507172584534\n",
      "        vf_loss: 4.209249973297119\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0398106575012207\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020541243255138397\n",
      "        policy_loss: -0.049791865050792694\n",
      "        total_loss: 3.9701037406921387\n",
      "        vf_explained_var: 0.45475006103515625\n",
      "        vf_loss: 4.0106520652771\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9929417371749878\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015462890267372131\n",
      "        policy_loss: -0.044401634484529495\n",
      "        total_loss: 4.368862628936768\n",
      "        vf_explained_var: 0.45643964409828186\n",
      "        vf_loss: 4.402826309204102\n",
      "    load_time_ms: 998.857\n",
      "    num_steps_sampled: 64512\n",
      "    num_steps_trained: 64512\n",
      "    sample_time_ms: 36835.72\n",
      "    update_time_ms: 3369.059\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 10.0.208.182\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.08032786885246\n",
      "    ram_util_percent: 43.79016393442624\n",
      "  pid: 117\n",
      "  policy_reward_max:\n",
      "    policy_0: 23.0\n",
      "    policy_1: 21.0\n",
      "    policy_2: 24.0\n",
      "    policy_3: 23.0\n",
      "    policy_4: 21.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 3.987190435525192\n",
      "    policy_1: 4.4534585824081985\n",
      "    policy_2: 4.5687446626814685\n",
      "    policy_3: 4.403928266438941\n",
      "    policy_4: 4.809564474807856\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.285290671963013\n",
      "    mean_inference_ms: 6.83734896933105\n",
      "    mean_processing_ms: 2.4626556184610253\n",
      "  time_since_restore: 379.78656792640686\n",
      "  time_this_iter_s: 47.23489451408386\n",
      "  time_total_s: 379.78656792640686\n",
      "  timestamp: 1645756857\n",
      "  timesteps_since_restore: 64512\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 64512\n",
      "  training_iteration: 7\n",
      "  trial_id: 5e0c2aea\n",
      "  worst_snake_episode_len_max: 21\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.4 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_5e0c2aea | RUNNING  | 10.0.208.182:117 |  22.2229 |          379.787 | 64512 |      7 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_5e0c2aea:\n",
      "  best_snake_episode_len_max: 27\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 8\n",
      "    Forbidden_move_mean: 3.7766990291262137\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 3\n",
      "    Killed_another_snake_mean: 0.32038834951456313\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 5\n",
      "    Snake_hit_body_mean: 0.6572815533980583\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 3\n",
      "    Snake_hit_wall_mean: 0.15922330097087378\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.7543689320388349\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 25\n",
      "    policy0_max_len_mean: 4.531067961165048\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 24\n",
      "    policy1_max_len_mean: 4.776699029126213\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 27\n",
      "    policy2_max_len_mean: 5.524271844660194\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 25\n",
      "    policy3_max_len_mean: 5.081553398058253\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 26\n",
      "    policy4_max_len_mean: 5.3174757281553395\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-02-25_02-41-45\n",
      "  done: false\n",
      "  episode_len_max: 27\n",
      "  episode_len_mean: 8.940776699029126\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 74.0\n",
      "  episode_reward_mean: 25.23106796116505\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 1030\n",
      "  episodes_total: 12751\n",
      "  experiment_id: 3217e5ecf36d427a81fa4f34f76a710e\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-208-182.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 12522.148\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0573400259017944\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016351789236068726\n",
      "        policy_loss: -0.04867732152342796\n",
      "        total_loss: 4.255589485168457\n",
      "        vf_explained_var: 0.46414729952812195\n",
      "        vf_loss: 4.293228626251221\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0187084674835205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020247500389814377\n",
      "        policy_loss: -0.04705072566866875\n",
      "        total_loss: 4.2704010009765625\n",
      "        vf_explained_var: 0.48225343227386475\n",
      "        vf_loss: 4.308340072631836\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9923025965690613\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012827337719500065\n",
      "        policy_loss: -0.0406314954161644\n",
      "        total_loss: 5.514730930328369\n",
      "        vf_explained_var: 0.4502852261066437\n",
      "        vf_loss: 5.5467047691345215\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0033715963363647\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01892703026533127\n",
      "        policy_loss: -0.04095236957073212\n",
      "        total_loss: 4.913361072540283\n",
      "        vf_explained_var: 0.47028154134750366\n",
      "        vf_loss: 4.941537857055664\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9448682069778442\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013665449805557728\n",
      "        policy_loss: -0.046544913202524185\n",
      "        total_loss: 4.637777328491211\n",
      "        vf_explained_var: 0.5171424150466919\n",
      "        vf_loss: 4.675098419189453\n",
      "    load_time_ms: 959.871\n",
      "    num_steps_sampled: 73728\n",
      "    num_steps_trained: 73728\n",
      "    sample_time_ms: 36794.869\n",
      "    update_time_ms: 2950.737\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 10.0.208.182\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.84126984126982\n",
      "    ram_util_percent: 43.83809523809525\n",
      "  pid: 117\n",
      "  policy_reward_max:\n",
      "    policy_0: 25.0\n",
      "    policy_1: 24.0\n",
      "    policy_2: 27.0\n",
      "    policy_3: 25.0\n",
      "    policy_4: 26.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 4.531067961165048\n",
      "    policy_1: 4.776699029126213\n",
      "    policy_2: 5.524271844660194\n",
      "    policy_3: 5.081553398058253\n",
      "    policy_4: 5.3174757281553395\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.30086701920153\n",
      "    mean_inference_ms: 6.864935723711407\n",
      "    mean_processing_ms: 2.4142502980209306\n",
      "  time_since_restore: 428.4763255119324\n",
      "  time_this_iter_s: 48.68975758552551\n",
      "  time_total_s: 428.4763255119324\n",
      "  timestamp: 1645756905\n",
      "  timesteps_since_restore: 73728\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 73728\n",
      "  training_iteration: 8\n",
      "  trial_id: 5e0c2aea\n",
      "  worst_snake_episode_len_max: 24\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.4 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_5e0c2aea | RUNNING  | 10.0.208.182:117 |  25.2311 |          428.476 | 73728 |      8 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_5e0c2aea:\n",
      "  best_snake_episode_len_max: 37\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 9\n",
      "    Forbidden_move_mean: 3.698717948717949\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 4\n",
      "    Killed_another_snake_mean: 0.33974358974358976\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 5\n",
      "    Snake_hit_body_mean: 0.7649572649572649\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 3\n",
      "    Snake_hit_wall_mean: 0.12072649572649573\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 6\n",
      "    Snake_was_eaten_mean: 0.7435897435897436\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 37\n",
      "    policy0_max_len_mean: 4.990384615384615\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 27\n",
      "    policy1_max_len_mean: 5.64957264957265\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 29\n",
      "    policy2_max_len_mean: 5.762820512820513\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 36\n",
      "    policy3_max_len_mean: 5.443376068376068\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 28\n",
      "    policy4_max_len_mean: 6.338675213675214\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-02-25_02-42-33\n",
      "  done: false\n",
      "  episode_len_max: 37\n",
      "  episode_len_mean: 9.847222222222221\n",
      "  episode_len_min: 2\n",
      "  episode_reward_max: 87.0\n",
      "  episode_reward_mean: 28.18482905982906\n",
      "  episode_reward_min: 3.0\n",
      "  episodes_this_iter: 936\n",
      "  episodes_total: 13687\n",
      "  experiment_id: 3217e5ecf36d427a81fa4f34f76a710e\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-208-182.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 12395.122\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.03572416305542\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014131490141153336\n",
      "        policy_loss: -0.040838588029146194\n",
      "        total_loss: 4.640922546386719\n",
      "        vf_explained_var: 0.4868365526199341\n",
      "        vf_loss: 4.67222261428833\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9506546854972839\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015233784914016724\n",
      "        policy_loss: -0.043318185955286026\n",
      "        total_loss: 5.044509410858154\n",
      "        vf_explained_var: 0.49801233410835266\n",
      "        vf_loss: 5.077544212341309\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9561238884925842\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013667084276676178\n",
      "        policy_loss: -0.04252178966999054\n",
      "        total_loss: 5.8571929931640625\n",
      "        vf_explained_var: 0.49158981442451477\n",
      "        vf_loss: 5.89048957824707\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9759150743484497\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01609577052295208\n",
      "        policy_loss: -0.046846337616443634\n",
      "        total_loss: 5.658686637878418\n",
      "        vf_explained_var: 0.4927595853805542\n",
      "        vf_loss: 5.694668292999268\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8920294642448425\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014926806092262268\n",
      "        policy_loss: -0.04411395639181137\n",
      "        total_loss: 5.4396185874938965\n",
      "        vf_explained_var: 0.5217100381851196\n",
      "        vf_loss: 5.47365665435791\n",
      "    load_time_ms: 929.072\n",
      "    num_steps_sampled: 82944\n",
      "    num_steps_trained: 82944\n",
      "    sample_time_ms: 36674.768\n",
      "    update_time_ms: 2625.576\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 10.0.208.182\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.3741935483871\n",
      "    ram_util_percent: 43.90483870967742\n",
      "  pid: 117\n",
      "  policy_reward_max:\n",
      "    policy_0: 37.0\n",
      "    policy_1: 27.0\n",
      "    policy_2: 29.0\n",
      "    policy_3: 36.0\n",
      "    policy_4: 28.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 4.990384615384615\n",
      "    policy_1: 5.64957264957265\n",
      "    policy_2: 5.762820512820513\n",
      "    policy_3: 5.443376068376068\n",
      "    policy_4: 6.338675213675214\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.312086346762606\n",
      "    mean_inference_ms: 6.87608642969686\n",
      "    mean_processing_ms: 2.3646081560699956\n",
      "  time_since_restore: 476.32542538642883\n",
      "  time_this_iter_s: 47.84909987449646\n",
      "  time_total_s: 476.32542538642883\n",
      "  timestamp: 1645756953\n",
      "  timesteps_since_restore: 82944\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 82944\n",
      "  training_iteration: 9\n",
      "  trial_id: 5e0c2aea\n",
      "  worst_snake_episode_len_max: 27\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.4 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_5e0c2aea | RUNNING  | 10.0.208.182:117 |  28.1848 |          476.325 | 82944 |      9 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_5e0c2aea:\n",
      "  best_snake_episode_len_max: 36\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 8\n",
      "    Forbidden_move_mean: 3.550724637681159\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 4\n",
      "    Killed_another_snake_mean: 0.3973429951690821\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 5\n",
      "    Snake_hit_body_mean: 0.7971014492753623\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 2\n",
      "    Snake_hit_wall_mean: 0.0821256038647343\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.8792270531400966\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 29\n",
      "    policy0_max_len_mean: 5.702898550724638\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 35\n",
      "    policy1_max_len_mean: 6.353864734299517\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 29\n",
      "    policy2_max_len_mean: 6.404589371980676\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 36\n",
      "    policy3_max_len_mean: 6.6727053140096615\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 34\n",
      "    policy4_max_len_mean: 6.61231884057971\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-02-25_02-43-20\n",
      "  done: true\n",
      "  episode_len_max: 36\n",
      "  episode_len_mean: 11.142512077294686\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 101.0\n",
      "  episode_reward_mean: 31.746376811594203\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 828\n",
      "  episodes_total: 14515\n",
      "  experiment_id: 3217e5ecf36d427a81fa4f34f76a710e\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-208-182.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 12265.868\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9763504862785339\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014508490450680256\n",
      "        policy_loss: -0.04428441822528839\n",
      "        total_loss: 5.765830039978027\n",
      "        vf_explained_var: 0.5106393098831177\n",
      "        vf_loss: 5.800321578979492\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9180115461349487\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014799486845731735\n",
      "        policy_loss: -0.043467678129673004\n",
      "        total_loss: 6.4794416427612305\n",
      "        vf_explained_var: 0.493984580039978\n",
      "        vf_loss: 6.512919902801514\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9175354838371277\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01379990205168724\n",
      "        policy_loss: -0.04135529324412346\n",
      "        total_loss: 7.243907451629639\n",
      "        vf_explained_var: 0.489019513130188\n",
      "        vf_loss: 7.275947570800781\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9209604263305664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014929132536053658\n",
      "        policy_loss: -0.04238204285502434\n",
      "        total_loss: 7.286141395568848\n",
      "        vf_explained_var: 0.4902229905128479\n",
      "        vf_loss: 7.318446636199951\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8934616446495056\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01410062424838543\n",
      "        policy_loss: -0.042666979134082794\n",
      "        total_loss: 6.583537578582764\n",
      "        vf_explained_var: 0.5320879220962524\n",
      "        vf_loss: 6.616686820983887\n",
      "    load_time_ms: 903.124\n",
      "    num_steps_sampled: 92160\n",
      "    num_steps_trained: 92160\n",
      "    sample_time_ms: 36488.868\n",
      "    update_time_ms: 2365.702\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 10.0.208.182\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.06721311475411\n",
      "    ram_util_percent: 43.929508196721315\n",
      "  pid: 117\n",
      "  policy_reward_max:\n",
      "    policy_0: 29.0\n",
      "    policy_1: 35.0\n",
      "    policy_2: 29.0\n",
      "    policy_3: 36.0\n",
      "    policy_4: 34.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 5.702898550724638\n",
      "    policy_1: 6.353864734299517\n",
      "    policy_2: 6.404589371980676\n",
      "    policy_3: 6.6727053140096615\n",
      "    policy_4: 6.61231884057971\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.3175025876999955\n",
      "    mean_inference_ms: 6.86379740604453\n",
      "    mean_processing_ms: 2.314443794602245\n",
      "  time_since_restore: 523.0108788013458\n",
      "  time_this_iter_s: 46.68545341491699\n",
      "  time_total_s: 523.0108788013458\n",
      "  timestamp: 1645757000\n",
      "  timesteps_since_restore: 92160\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 92160\n",
      "  training_iteration: 10\n",
      "  trial_id: 5e0c2aea\n",
      "  worst_snake_episode_len_max: 29\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.4 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_5e0c2aea | RUNNING  | 10.0.208.182:117 |  31.7464 |          523.011 | 92160 |     10 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.6/15.4 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 0/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 TERMINATED)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+------------+-------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status     | loc   |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+------------+-------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_5e0c2aea | TERMINATED |       |  31.7464 |          523.011 | 92160 |     10 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+------------+-------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34mSaved model configuration.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mSaved the checkpoint file /opt/ml/output/intermediate/training/PPO_MultiAgentBattlesnake-v1_5e0c2aea_0_2022-02-25_02-33-50a3hcdivz/checkpoint_10/checkpoint-10 as /opt/ml/model/checkpoint\u001b[0m\n",
      "\u001b[34mSaved the checkpoint file /opt/ml/output/intermediate/training/PPO_MultiAgentBattlesnake-v1_5e0c2aea_0_2022-02-25_02-33-50a3hcdivz/checkpoint_10/checkpoint-10.tune_metadata as /opt/ml/model/checkpoint.tune_metadata\u001b[0m\n",
      "\u001b[34m2022-02-25 02:43:25,769#011INFO trainer.py:420 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\u001b[0m\n",
      "\u001b[34m2022-02-25 02:43:25,778#011INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1044)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1044)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1044)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1044)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1044)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1044)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1044)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1044)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1044)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1044)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m2022-02-25 02:43:57,237#011INFO trainable.py:178 -- _setup took 31.461 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\u001b[0m\n",
      "\u001b[34m2022-02-25 02:43:59,088#011WARNING trainable.py:210 -- Getting current IP.\u001b[0m\n",
      "\u001b[34m2022-02-25 02:43:59,088#011INFO trainable.py:416 -- Restored on 10.0.208.182 from checkpoint: /opt/ml/model/checkpoint\u001b[0m\n",
      "\u001b[34m2022-02-25 02:43:59,088#011INFO trainable.py:423 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': 92160, '_time_total': 523.0108788013458, '_episodes_total': 14515}\u001b[0m\n",
      "\u001b[34mSaved TensorFlow serving model!\u001b[0m\n",
      "\u001b[34m2022-02-25 02:44:04,814 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-02-25 02:44:18 Uploading - Uploading generated training model\n",
      "2022-02-25 02:44:18 Completed - Training job completed\n",
      "Training seconds: 708\n",
      "Billable seconds: 708\n",
      "Training job: sagemaker-soln-bs-job-rllib-2022-02-25-02-29-53-333\n",
      "CPU times: user 1.83 s, sys: 139 ms, total: 1.97 s\n",
      "Wall time: 14min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define and execute our training job\n",
    "# Adjust hyperparameters and train_instance_count accordingly\n",
    "\n",
    "metric_definitions =  [\n",
    "    {'Name': 'training_iteration', 'Regex': 'training_iteration: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episodes_total', 'Regex': 'episodes_total: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'num_steps_trained', 'Regex': 'num_steps_trained: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'timesteps_total', 'Regex': 'timesteps_total: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'training_iteration', 'Regex': 'training_iteration: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "\n",
    "    {'Name': 'episode_reward_max', 'Regex': 'episode_reward_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episode_reward_mean', 'Regex': 'episode_reward_mean: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episode_reward_min', 'Regex': 'episode_reward_min: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    \n",
    "    {'Name': 'episode_len_max', 'Regex': 'episode_len_mean: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episode_len_mean', 'Regex': 'episode_len_mean: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episode_len_min', 'Regex': 'episode_len_mean: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "\n",
    "    {'Name': 'best_snake_episode_len_max', 'Regex': 'best_snake_episode_len_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'worst_snake_episode_len_max', 'Regex': 'worst_snake_episode_len_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "\n",
    "    {'Name': 'Snake_hit_wall_max', 'Regex': 'Snake_hit_wall_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'Snake_was_eaten_max', 'Regex': 'Snake_was_eaten_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'Killed_another_snake_max', 'Regex': 'Killed_another_snake_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'Snake_hit_body_max', 'Regex': 'Snake_hit_body_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'Starved_max', 'Regex': 'Starved_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'Forbidden_move_max', 'Regex': 'Forbidden_move_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}\n",
    "] \n",
    "\n",
    "algorithm = \"PPO\"\n",
    "map_size = 11\n",
    "num_agents = 5\n",
    "additional_config = {\n",
    "    'lambda': 0.90,\n",
    "    'gamma': 0.999,\n",
    "    'kl_coeff': 0.2,\n",
    "    'clip_rewards': True,\n",
    "    'vf_clip_param': 175.0,\n",
    "    'train_batch_size': 9216,\n",
    "    'sample_batch_size': 96,\n",
    "    'sgd_minibatch_size': 256,\n",
    "    'num_sgd_iter': 3,\n",
    "    'lr': 5.0e-4,\n",
    "}\n",
    "\n",
    "estimator = RLEstimator(entry_point=\"train-mabs.py\",\n",
    "                        source_dir='training/training_src',\n",
    "                        dependencies=[\"training/common/sagemaker_rl\", \"inference/inference_src/\", \"../BattlesnakeGym/\"],\n",
    "                        image_uri=image_name,\n",
    "                        role=role,\n",
    "                        train_instance_type=instance_type,\n",
    "                        train_instance_count=1,\n",
    "                        output_path=s3_output_path,\n",
    "                        base_job_name=job_name_prefix,\n",
    "                        metric_definitions=metric_definitions,\n",
    "                        hyperparameters={\n",
    "                            # See train-mabs.py to add additional hyperparameters\n",
    "                            # Also see ray_launcher.py for the rl.training.* hyperparameters\n",
    "                            \n",
    "                            \"num_iters\": 10,\n",
    "                            # number of snakes in the gym\n",
    "                            \"num_agents\": num_agents,\n",
    "\n",
    "                            \"iterate_map_size\": False,\n",
    "                            \"map_size\": map_size,\n",
    "                            \"algorithm\": algorithm,\n",
    "                            \"additional_configs\": additional_config,\n",
    "                            \"use_heuristics_action_masks\": False\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "estimator.fit()\n",
    "\n",
    "job_name = estimator.latest_training_job.job_name\n",
    "print(\"Training job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-soln-bs-rbcsnake-bucket/sagemaker-soln-bs-job-rllib-2022-02-25-02-29-53-333/output/model.tar.gz'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Where is the model stored in S3?\n",
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an endpoint to host the policy\n",
    "Firstly, we will delete the previous endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 6.2 MiB/6.2 MiB (32.9 MiB/s) with 1 file(s) remaining\r",
      "copy: s3://sagemaker-soln-bs-rbcsnake-bucket/sagemaker-soln-bs-job-rllib-2022-02-25-02-29-53-333/output/model.tar.gz to s3://sagemaker-soln-bs-rbcsnake-bucket/pretrainedmodels/model.tar.gz\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.tensorflow.serving.Model has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=info['SagemakerEndPointName'])\n",
    "try:\n",
    "    sm_client.delete_endpoint(EndpointName=info['SagemakerEndPointName'])\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=info['SagemakerEndPointName'])\n",
    "    sm_client.delete_model(ModelName=info['SagemakerEndPointName'])\n",
    "    ep_waiter = sm_client.get_waiter('endpoint_deleted')\n",
    "    ep_waiter.wait(EndpointName=info['SagemakerEndPointName'])\n",
    "except botocore.exceptions.ClientError:\n",
    "    pass\n",
    "    \n",
    "# Copy the endpoint to a central location\n",
    "model_data = \"s3://{}/pretrainedmodels/model.tar.gz\".format(s3_bucket)\n",
    "!aws s3 cp {estimator.model_data} {model_data}\n",
    "\n",
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "model = Model(model_data=model_data,\n",
    "              role=role,\n",
    "              entry_point=\"inference.py\",\n",
    "              source_dir='inference/inference_src',\n",
    "              framework_version='2.1.0',\n",
    "              name=info['SagemakerEndPointName'],\n",
    "              code_location='s3://{}//code'.format(s3_bucket)\n",
    "             )\n",
    "\n",
    "if local_mode:\n",
    "    inf_instance_type = 'local'\n",
    "else:\n",
    "    inf_instance_type = info[\"SagemakerInferenceInstanceType\"]\n",
    "\n",
    "# Deploy an inference endpoint\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=inf_instance_type,\n",
    "                         endpoint_name=info['SagemakerEndPointName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the endpoint\n",
    "\n",
    "This example is using single observation for a 5-agent environment \n",
    "The last axis is 12 because the current MultiAgentEnv is concatenating 2 frames\n",
    "5 agent maps + 1 food map = 6 maps total    6 maps * 2 frames = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action to take 0\n",
      "Inference took 752.89 ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "state = np.zeros(shape=(1, 21, 21, 6), dtype=np.float32).tolist()\n",
    "\n",
    "health_dict = {0: 50, 1: 50}\n",
    "json = {\"turn\": 4,\n",
    "        \"board\": {\n",
    "                \"height\": 11,\n",
    "                \"width\": 11,\n",
    "                \"food\": [],\n",
    "                \"snakes\": []\n",
    "                },\n",
    "            \"you\": {\n",
    "                \"id\": \"snake-id-string\",\n",
    "                \"name\": \"Sneky Snek\",\n",
    "                \"health\": 90,\n",
    "                \"body\": [{\"x\": 1, \"y\": 3}]\n",
    "                }\n",
    "            }\n",
    "\n",
    "before = time()\n",
    "action_mask = np.array([1, 1, 1, 1]).tolist()\n",
    "\n",
    "action = predictor.predict({\"state\": state, \"action_mask\": action_mask,\n",
    "                            \"prev_action\": -1, \n",
    "                           \"prev_reward\": -1, \"seq_lens\": -1,  \n",
    "                           \"all_health\": health_dict, \"json\": json})\n",
    "elapsed = time() - before\n",
    "\n",
    "action_to_take = action[\"outputs\"][\"heuristisc_action\"]\n",
    "print(\"Action to take {}\".format(action_to_take))\n",
    "print(\"Inference took %.2f ms\" % (elapsed*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "- To go back to the introduction click [here](./1_Introduction.ipynb)\n",
    "- To build some heuristics click [here](./3_HeuristicsDeveloper.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
