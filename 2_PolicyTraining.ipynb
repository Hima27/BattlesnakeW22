{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: When running this notebook on SageMaker Studio, you should make sure the 'SageMaker JumpStart Tensorflow 1.0' image/kernel is used. You can run run all cells at once or step through the notebook.\n",
    "# Policy Training\n",
    "\n",
    "This notebook outlines the steps involved in building and deploying a Battlesnake model using Ray RLlib and TensorFlow on Amazon SageMaker.\n",
    "\n",
    "Library versions currently in use:  TensorFlow 2.1, Ray RLlib 0.8.2\n",
    "\n",
    "The model is first trained using multi-agent PPO, and then deployed to a managed _TensorFlow Serving_ SageMaker endpoint that can be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.rl import RLEstimator, RLToolkit\n",
    "import boto3\n",
    "import botocore\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AwsAccountId': '018864217387', 'AwsRegion': 'us-west-2', 'S3Bucket': 'sagemaker-soln-bs-rbcsnake-bucket', 'SolutionPrefix': 'sagemaker-soln-bs', 'SageMakerIamRoleArn': 'arn:aws:iam::018864217387:role/sagemaker-soln-bs-us-west-2-nb-role', 'SnakeAPI': 'https://56fbd5ro6k.execute-api.us-west-2.amazonaws.com/snake/', 'EndPointS3Location': 's3://sagemaker-solutions-prod-us-west-2/sagemaker-battlesnake-ai/1.1.0/build/model-complete.tar.gz', 'SagemakerEndPointName': 'sagemaker-soln-bs-ep', 'SagemakerTrainingInstanceType': 'ml.m5.xlarge', 'SagemakerInferenceInstanceType': 'ml.t2.medium'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../stack_outputs.json\") as f:\n",
    "    info = json.load(f)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise sagemaker\n",
    "We need to define several parameters prior to running the training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket path: s3://sagemaker-soln-bs-rbcsnake-bucket/\n"
     ]
    }
   ],
   "source": [
    "sm_session = sagemaker.session.Session()\n",
    "s3_bucket = info[\"S3Bucket\"]\n",
    "\n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::018864217387:role/sagemaker-soln-bs-us-west-2-nb-role\n"
     ]
    }
   ],
   "source": [
    "job_name_prefix = info[\"SolutionPrefix\"]+'-job-rllib'\n",
    "\n",
    "role = info[\"SageMakerIamRoleArn\"]\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change local_mode to True if you want to do local training within this Notebook instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    instance_type = 'local'\n",
    "else:\n",
    "    instance_type = info[\"SagemakerTrainingInstanceType\"]\n",
    "    \n",
    "# If training locally, do some Docker housekeeping..\n",
    "if local_mode:\n",
    "    !/bin/bash ./common/setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sm_session.boto_region_name\n",
    "device = \"cpu\"\n",
    "image_name = '462105765813.dkr.ecr.{region}.amazonaws.com/sagemaker-rl-ray-container:ray-0.8.2-tf-{device}-py36'.format(region=region, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-02 20:06:52 Starting - Starting the training job...\n",
      "2022-03-02 20:07:17 Starting - Launching requested ML instancesProfilerReport-1646251612: InProgress\n",
      ".........\n",
      "2022-03-02 20:08:37 Starting - Preparing the instances for training...\n",
      "2022-03-02 20:09:19 Downloading - Downloading input data\n",
      "2022-03-02 20:09:19 Training - Downloading the training image......\n",
      "2022-03-02 20:10:23 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-03-02 20:10:27,965 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2022-03-02 20:10:27,972 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-03-02 20:10:28,160 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting array2gif\n",
      "  Downloading array2gif-1.0.4-py3-none-any.whl (7.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting mxboard\n",
      "  Downloading mxboard-0.1.0-py3-none-any.whl (47 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from array2gif->-r requirements.txt (line 1)) (1.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from mxboard->-r requirements.txt (line 2)) (7.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from mxboard->-r requirements.txt (line 2)) (3.11.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from mxboard->-r requirements.txt (line 2)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->-r requirements.txt (line 3)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->-r requirements.txt (line 3)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym->-r requirements.txt (line 3)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.0.0->mxboard->-r requirements.txt (line 2)) (46.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->-r requirements.txt (line 3)) (1.25.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->-r requirements.txt (line 3)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->-r requirements.txt (line 3)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->-r requirements.txt (line 3)) (2020.4.5.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: battlesnake-gym\n",
      "  Building wheel for battlesnake-gym (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for battlesnake-gym (setup.py): finished with status 'done'\n",
      "  Created wheel for battlesnake-gym: filename=battlesnake_gym-0.1.dev0-py3-none-any.whl size=16942 sha256=37b58a8056941d9ac8f18817ec3c115a8c760befb5842cf3851523868582d1bf\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cmbj3ck4/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca\u001b[0m\n",
      "\u001b[34mSuccessfully built battlesnake-gym\u001b[0m\n",
      "\u001b[34mInstalling collected packages: array2gif, mxboard, battlesnake-gym\u001b[0m\n",
      "\u001b[34mSuccessfully installed array2gif-1.0.4 battlesnake-gym-0.1.dev0 mxboard-0.1.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.2; however, version 21.3.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2022-03-02 20:10:30,382 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-03-02 20:10:30,399 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-03-02 20:10:30,414 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-03-02 20:10:30,424 sagemaker-containers INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_estimator\": \"RLEstimator\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"num_agents\": 5,\n",
      "        \"use_heuristics_action_masks\": false,\n",
      "        \"additional_configs\": {\n",
      "            \"lambda\": 0.9,\n",
      "            \"gamma\": 0.999,\n",
      "            \"kl_coeff\": 0.2,\n",
      "            \"clip_rewards\": true,\n",
      "            \"vf_clip_param\": 175.0,\n",
      "            \"train_batch_size\": 9216,\n",
      "            \"sample_batch_size\": 96,\n",
      "            \"sgd_minibatch_size\": 256,\n",
      "            \"num_sgd_iter\": 3,\n",
      "            \"lr\": 0.0005\n",
      "        },\n",
      "        \"map_size\": 11,\n",
      "        \"num_iters\": 10,\n",
      "        \"iterate_map_size\": false,\n",
      "        \"algorithm\": \"PPO\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-soln-bs-job-rllib-2022-03-02-20-06-52-405\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-soln-bs-rbcsnake-bucket/sagemaker-soln-bs-job-rllib-2022-03-02-20-06-52-405/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-mabs\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-mabs.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"additional_configs\":{\"clip_rewards\":true,\"gamma\":0.999,\"kl_coeff\":0.2,\"lambda\":0.9,\"lr\":0.0005,\"num_sgd_iter\":3,\"sample_batch_size\":96,\"sgd_minibatch_size\":256,\"train_batch_size\":9216,\"vf_clip_param\":175.0},\"algorithm\":\"PPO\",\"iterate_map_size\":false,\"map_size\":11,\"num_agents\":5,\"num_iters\":10,\"use_heuristics_action_masks\":false}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-mabs.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_estimator\":\"RLEstimator\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-mabs\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-soln-bs-rbcsnake-bucket/sagemaker-soln-bs-job-rllib-2022-03-02-20-06-52-405/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_estimator\":\"RLEstimator\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"additional_configs\":{\"clip_rewards\":true,\"gamma\":0.999,\"kl_coeff\":0.2,\"lambda\":0.9,\"lr\":0.0005,\"num_sgd_iter\":3,\"sample_batch_size\":96,\"sgd_minibatch_size\":256,\"train_batch_size\":9216,\"vf_clip_param\":175.0},\"algorithm\":\"PPO\",\"iterate_map_size\":false,\"map_size\":11,\"num_agents\":5,\"num_iters\":10,\"use_heuristics_action_masks\":false},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-soln-bs-job-rllib-2022-03-02-20-06-52-405\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-soln-bs-rbcsnake-bucket/sagemaker-soln-bs-job-rllib-2022-03-02-20-06-52-405/source/sourcedir.tar.gz\",\"module_name\":\"train-mabs\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-mabs.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--additional_configs\",\"clip_rewards=True,gamma=0.999,kl_coeff=0.2,lambda=0.9,lr=0.0005,num_sgd_iter=3,sample_batch_size=96,sgd_minibatch_size=256,train_batch_size=9216,vf_clip_param=175.0\",\"--algorithm\",\"PPO\",\"--iterate_map_size\",\"False\",\"--map_size\",\"11\",\"--num_agents\",\"5\",\"--num_iters\",\"10\",\"--use_heuristics_action_masks\",\"False\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_AGENTS=5\u001b[0m\n",
      "\u001b[34mSM_HP_USE_HEURISTICS_ACTION_MASKS=false\u001b[0m\n",
      "\u001b[34mSM_HP_ADDITIONAL_CONFIGS={\"clip_rewards\":true,\"gamma\":0.999,\"kl_coeff\":0.2,\"lambda\":0.9,\"lr\":0.0005,\"num_sgd_iter\":3,\"sample_batch_size\":96,\"sgd_minibatch_size\":256,\"train_batch_size\":9216,\"vf_clip_param\":175.0}\u001b[0m\n",
      "\u001b[34mSM_HP_MAP_SIZE=11\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_ITERS=10\u001b[0m\n",
      "\u001b[34mSM_HP_ITERATE_MAP_SIZE=false\u001b[0m\n",
      "\u001b[34mSM_HP_ALGORITHM=PPO\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 -m train-mabs --additional_configs clip_rewards=True,gamma=0.999,kl_coeff=0.2,lambda=0.9,lr=0.0005,num_sgd_iter=3,sample_batch_size=96,sgd_minibatch_size=256,train_batch_size=9216,vf_clip_param=175.0 --algorithm PPO --iterate_map_size False --map_size 11 --num_agents 5 --num_iters 10 --use_heuristics_action_masks False\u001b[0m\n",
      "\u001b[34m2022-03-02 20:10:33,441#011INFO resource_spec.py:212 -- Starting Ray with 6.69 GiB memory available for workers and up to 3.36 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\u001b[0m\n",
      "\u001b[34m2022-03-02 20:10:33,846#011INFO services.py:1078 -- View the Ray dashboard at #033[1m#033[32mlocalhost:8265#033[39m#033[22m\u001b[0m\n",
      "\u001b[34mNo checkpoint path specified. Training from scratch.\u001b[0m\n",
      "\u001b[34mImportant! Ray with version <=7.2 may report \"Did not find checkpoint file\" even if the experiment is actually restored successfully. If restoration is expected, please check \"training_iteration\" in the experiment info to confirm.\u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 2.3/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+-------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc   |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+-------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_d1764e86 | RUNNING  |       |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+-------+\u001b[0m\n",
      "\u001b[34m2022-03-02 20:10:35,706#011WARNING worker.py:1058 -- The dashboard on node ip-10-0-251-116.us-west-2.compute.internal failed with the following error:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1062, in create_server\n",
      "    sock.bind(sa)\u001b[0m\n",
      "\u001b[34mOSError: [Errno 99] Cannot assign requested address\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/dashboard/dashboard.py\", line 920, in <module>\n",
      "    dashboard.run()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/dashboard/dashboard.py\", line 368, in run\n",
      "    aiohttp.web.run_app(self.app, host=self.host, port=self.port)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web.py\", line 433, in run_app\n",
      "    reuse_port=reuse_port))\n",
      "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 484, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web.py\", line 359, in _run_app\n",
      "    await site.start()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web_runner.py\", line 104, in start\n",
      "    reuse_port=self._reuse_port)\n",
      "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1066, in create_server\n",
      "    % (sa, err.strerror.lower()))\u001b[0m\n",
      "\u001b[34mOSError: [Errno 99] error while attempting to bind on address ('::1', 8265, 0, 0): cannot assign requested address\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=111)#033[0m 2022-03-02 20:10:37,554#011INFO trainer.py:420 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=111)#033[0m 2022-03-02 20:10:37,556#011INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=111)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=111)#033[0m   obj = yaml.load(type_)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#033[2m#033[36m(pid=111)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=111)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=111)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=111)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=111)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=111)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=111)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=111)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=112)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=112)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=110)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=110)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=112)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=112)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=110)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=110)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=110)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=110)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=112)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=112)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=110)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=110)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=112)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=112)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=112)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=112)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=110)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=110)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=111)#033[0m 2022-03-02 20:11:18,210#011INFO trainable.py:178 -- _setup took 40.656 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_d1764e86:\n",
      "  best_snake_episode_len_max: 14\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 10\n",
      "    Forbidden_move_mean: 3.819858156028369\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 2\n",
      "    Killed_another_snake_mean: 0.1111111111111111\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 4\n",
      "    Snake_hit_body_mean: 0.24302600472813238\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 7\n",
      "    Snake_hit_wall_mean: 1.4264775413711583\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.28747044917257686\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 14\n",
      "    policy0_max_len_mean: 2.2099290780141843\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 14\n",
      "    policy1_max_len_mean: 2.345626477541371\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 12\n",
      "    policy2_max_len_mean: 2.2141843971631205\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 13\n",
      "    policy3_max_len_mean: 2.258628841607565\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 14\n",
      "    policy4_max_len_mean: 2.273758865248227\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-03-02_20-12-50\n",
      "  done: false\n",
      "  episode_len_max: 15\n",
      "  episode_len_mean: 4.353191489361702\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 40.0\n",
      "  episode_reward_mean: 11.302127659574468\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2115\n",
      "  episodes_total: 2115\n",
      "  experiment_id: d48644d2bc65443c8bfbbc4cf16746cc\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-251-116.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 20561.144\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3739062547683716\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010712740942835808\n",
      "        policy_loss: -0.016688687726855278\n",
      "        total_loss: 1.446578860282898\n",
      "        vf_explained_var: 0.18736377358436584\n",
      "        vf_loss: 1.4611250162124634\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3706430196762085\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012365913949906826\n",
      "        policy_loss: -0.024625292047858238\n",
      "        total_loss: 1.6451348066329956\n",
      "        vf_explained_var: 0.17249685525894165\n",
      "        vf_loss: 1.667286992073059\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.375678539276123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00997474417090416\n",
      "        policy_loss: -0.022053562104701996\n",
      "        total_loss: 1.462515115737915\n",
      "        vf_explained_var: 0.19967055320739746\n",
      "        vf_loss: 1.4825738668441772\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3728879690170288\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009668930433690548\n",
      "        policy_loss: -0.016320550814270973\n",
      "        total_loss: 1.5127002000808716\n",
      "        vf_explained_var: 0.15851178765296936\n",
      "        vf_loss: 1.5270867347717285\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3739590644836426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011084377765655518\n",
      "        policy_loss: -0.013668404892086983\n",
      "        total_loss: 1.6810016632080078\n",
      "        vf_explained_var: 0.14743643999099731\n",
      "        vf_loss: 1.692453145980835\n",
      "    load_time_ms: 2891.525\n",
      "    num_steps_sampled: 9216\n",
      "    num_steps_trained: 9216\n",
      "    sample_time_ms: 39688.876\n",
      "    update_time_ms: 24610.263\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.251.116\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.96904761904762\n",
      "    ram_util_percent: 34.62777777777777\n",
      "  pid: 111\n",
      "  policy_reward_max:\n",
      "    policy_0: 14.0\n",
      "    policy_1: 14.0\n",
      "    policy_2: 12.0\n",
      "    policy_3: 13.0\n",
      "    policy_4: 14.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 2.2099290780141843\n",
      "    policy_1: 2.345626477541371\n",
      "    policy_2: 2.2141843971631205\n",
      "    policy_3: 2.258628841607565\n",
      "    policy_4: 2.273758865248227\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.160081169768109\n",
      "    mean_inference_ms: 7.334431867538467\n",
      "    mean_processing_ms: 2.6959761024154854\n",
      "  time_since_restore: 89.42247366905212\n",
      "  time_this_iter_s: 89.42247366905212\n",
      "  time_total_s: 89.42247366905212\n",
      "  timestamp: 1646251970\n",
      "  timesteps_since_restore: 9216\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 9216\n",
      "  training_iteration: 1\n",
      "  trial_id: d1764e86\n",
      "  worst_snake_episode_len_max: 12\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |   ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_d1764e86 | RUNNING  | 10.0.251.116:111 |  11.3021 |          89.4225 | 9216 |      1 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+------+--------+\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_d1764e86:\n",
      "  best_snake_episode_len_max: 14\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 10\n",
      "    Forbidden_move_mean: 3.783977110157368\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 3\n",
      "    Killed_another_snake_mean: 0.12875536480686695\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 6\n",
      "    Snake_hit_body_mean: 0.27801621363853124\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 8\n",
      "    Snake_hit_wall_mean: 1.3218884120171674\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.3562231759656652\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 12\n",
      "    policy0_max_len_mean: 2.2460658082975677\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 11\n",
      "    policy1_max_len_mean: 2.290891750119218\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 13\n",
      "    policy2_max_len_mean: 2.2889842632331905\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 12\n",
      "    policy3_max_len_mean: 2.291845493562232\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 14\n",
      "    policy4_max_len_mean: 2.296137339055794\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-03-02_20-13-41\n",
      "  done: false\n",
      "  episode_len_max: 14\n",
      "  episode_len_mean: 4.395326657129233\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 42.0\n",
      "  episode_reward_mean: 11.413924654268001\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 2097\n",
      "  episodes_total: 4212\n",
      "  experiment_id: d48644d2bc65443c8bfbbc4cf16746cc\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-251-116.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 15949.467\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3578797578811646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01266615092754364\n",
      "        policy_loss: -0.0304423701018095\n",
      "        total_loss: 1.4974806308746338\n",
      "        vf_explained_var: 0.3019930124282837\n",
      "        vf_loss: 1.5253899097442627\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3377593755722046\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01820623129606247\n",
      "        policy_loss: -0.03997718170285225\n",
      "        total_loss: 1.5279176235198975\n",
      "        vf_explained_var: 0.2953011989593506\n",
      "        vf_loss: 1.564253807067871\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3436040878295898\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019314611330628395\n",
      "        policy_loss: -0.04381416738033295\n",
      "        total_loss: 1.5667623281478882\n",
      "        vf_explained_var: 0.29400262236595154\n",
      "        vf_loss: 1.6067136526107788\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3499137163162231\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012824335135519505\n",
      "        policy_loss: -0.026817437261343002\n",
      "        total_loss: 1.574088215827942\n",
      "        vf_explained_var: 0.2834925949573517\n",
      "        vf_loss: 1.5983407497406006\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3578206300735474\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01271893922239542\n",
      "        policy_loss: -0.030198335647583008\n",
      "        total_loss: 1.636093020439148\n",
      "        vf_explained_var: 0.2792273163795471\n",
      "        vf_loss: 1.6637476682662964\n",
      "    load_time_ms: 1793.092\n",
      "    num_steps_sampled: 18432\n",
      "    num_steps_trained: 18432\n",
      "    sample_time_ms: 39154.929\n",
      "    update_time_ms: 12323.723\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.0.251.116\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.32575757575756\n",
      "    ram_util_percent: 43.94545454545454\n",
      "  pid: 111\n",
      "  policy_reward_max:\n",
      "    policy_0: 12.0\n",
      "    policy_1: 11.0\n",
      "    policy_2: 13.0\n",
      "    policy_3: 12.0\n",
      "    policy_4: 14.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 2.2460658082975677\n",
      "    policy_1: 2.290891750119218\n",
      "    policy_2: 2.2889842632331905\n",
      "    policy_3: 2.291845493562232\n",
      "    policy_4: 2.296137339055794\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.2017859415448338\n",
      "    mean_inference_ms: 7.278224910178679\n",
      "    mean_processing_ms: 2.731645204613182\n",
      "  time_since_restore: 140.37967777252197\n",
      "  time_this_iter_s: 50.95720410346985\n",
      "  time_total_s: 140.37967777252197\n",
      "  timestamp: 1646252021\n",
      "  timesteps_since_restore: 18432\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 18432\n",
      "  training_iteration: 2\n",
      "  trial_id: d1764e86\n",
      "  worst_snake_episode_len_max: 11\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_d1764e86 | RUNNING  | 10.0.251.116:111 |  11.4139 |           140.38 | 18432 |      2 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_d1764e86:\n",
      "  best_snake_episode_len_max: 18\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 10\n",
      "    Forbidden_move_mean: 3.9527518172377984\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 3\n",
      "    Killed_another_snake_mean: 0.1308411214953271\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 5\n",
      "    Snake_hit_body_mean: 0.27414330218068533\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 6\n",
      "    Snake_hit_wall_mean: 1.0939771547248183\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.31360332294911736\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 17\n",
      "    policy0_max_len_mean: 2.432502596053998\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 14\n",
      "    policy1_max_len_mean: 2.666147455867082\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 18\n",
      "    policy2_max_len_mean: 2.5425752855659396\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 14\n",
      "    policy3_max_len_mean: 2.4823468328141227\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 14\n",
      "    policy4_max_len_mean: 2.439252336448598\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-03-02_20-14-32\n",
      "  done: false\n",
      "  episode_len_max: 18\n",
      "  episode_len_mean: 4.7860851505711315\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 40.0\n",
      "  episode_reward_mean: 12.56282450674974\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 1926\n",
      "  episodes_total: 6138\n",
      "  experiment_id: d48644d2bc65443c8bfbbc4cf16746cc\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-251-116.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 14440.124\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3181089162826538\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02148294262588024\n",
      "        policy_loss: -0.04798907786607742\n",
      "        total_loss: 1.7694576978683472\n",
      "        vf_explained_var: 0.3445016145706177\n",
      "        vf_loss: 1.81315016746521\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.279510736465454\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025678597390651703\n",
      "        policy_loss: -0.05599392205476761\n",
      "        total_loss: 2.069587230682373\n",
      "        vf_explained_var: 0.3224591612815857\n",
      "        vf_loss: 2.120445489883423\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.290968894958496\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025486059486865997\n",
      "        policy_loss: -0.04910013824701309\n",
      "        total_loss: 1.9063903093338013\n",
      "        vf_explained_var: 0.3174605369567871\n",
      "        vf_loss: 1.9503930807113647\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3064109086990356\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0200357548892498\n",
      "        policy_loss: -0.04404187574982643\n",
      "        total_loss: 1.7747597694396973\n",
      "        vf_explained_var: 0.3397616147994995\n",
      "        vf_loss: 1.8147945404052734\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3212554454803467\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021000221371650696\n",
      "        policy_loss: -0.05145390331745148\n",
      "        total_loss: 1.9159132242202759\n",
      "        vf_explained_var: 0.3282192051410675\n",
      "        vf_loss: 1.9631671905517578\n",
      "    load_time_ms: 1439.131\n",
      "    num_steps_sampled: 27648\n",
      "    num_steps_trained: 27648\n",
      "    sample_time_ms: 38754.351\n",
      "    update_time_ms: 8222.86\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.0.251.116\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.96153846153847\n",
      "    ram_util_percent: 44.01692307692308\n",
      "  pid: 111\n",
      "  policy_reward_max:\n",
      "    policy_0: 17.0\n",
      "    policy_1: 14.0\n",
      "    policy_2: 18.0\n",
      "    policy_3: 14.0\n",
      "    policy_4: 14.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 2.432502596053998\n",
      "    policy_1: 2.666147455867082\n",
      "    policy_2: 2.5425752855659396\n",
      "    policy_3: 2.4823468328141227\n",
      "    policy_4: 2.439252336448598\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.2071451697250852\n",
      "    mean_inference_ms: 7.244467891528269\n",
      "    mean_processing_ms: 2.6913724076588776\n",
      "  time_since_restore: 190.58465695381165\n",
      "  time_this_iter_s: 50.20497918128967\n",
      "  time_total_s: 190.58465695381165\n",
      "  timestamp: 1646252072\n",
      "  timesteps_since_restore: 27648\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 27648\n",
      "  training_iteration: 3\n",
      "  trial_id: d1764e86\n",
      "  worst_snake_episode_len_max: 14\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_d1764e86 | RUNNING  | 10.0.251.116:111 |  12.5628 |          190.585 | 27648 |      3 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_d1764e86:\n",
      "  best_snake_episode_len_max: 18\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 8\n",
      "    Forbidden_move_mean: 4.143020594965675\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 3\n",
      "    Killed_another_snake_mean: 0.17791762013729978\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 5\n",
      "    Snake_hit_body_mean: 0.35068649885583525\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 5\n",
      "    Snake_hit_wall_mean: 0.755720823798627\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.40274599542334094\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 15\n",
      "    policy0_max_len_mean: 2.747711670480549\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 18\n",
      "    policy1_max_len_mean: 2.977688787185355\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 16\n",
      "    policy2_max_len_mean: 2.9302059496567505\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 18\n",
      "    policy3_max_len_mean: 2.679633867276888\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 15\n",
      "    policy4_max_len_mean: 2.696796338672769\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-03-02_20-15-21\n",
      "  done: false\n",
      "  episode_len_max: 19\n",
      "  episode_len_mean: 5.268878718535469\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 45.0\n",
      "  episode_reward_mean: 14.03203661327231\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 1748\n",
      "  episodes_total: 7886\n",
      "  experiment_id: d48644d2bc65443c8bfbbc4cf16746cc\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-251-116.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 13664.916\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2743536233901978\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02216319553554058\n",
      "        policy_loss: -0.047892823815345764\n",
      "        total_loss: 2.308901071548462\n",
      "        vf_explained_var: 0.3448450267314911\n",
      "        vf_loss: 2.350144863128662\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2026820182800293\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023025626316666603\n",
      "        policy_loss: -0.0508989542722702\n",
      "        total_loss: 2.626513719558716\n",
      "        vf_explained_var: 0.3315451443195343\n",
      "        vf_loss: 2.6705055236816406\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.214935302734375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.026542827486991882\n",
      "        policy_loss: -0.05663381144404411\n",
      "        total_loss: 2.4229180812835693\n",
      "        vf_explained_var: 0.33515945076942444\n",
      "        vf_loss: 2.4715890884399414\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2241662740707397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022834738716483116\n",
      "        policy_loss: -0.054517216980457306\n",
      "        total_loss: 2.153524160385132\n",
      "        vf_explained_var: 0.3597930073738098\n",
      "        vf_loss: 2.201190710067749\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2625545263290405\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022644273936748505\n",
      "        policy_loss: -0.0521588996052742\n",
      "        total_loss: 2.2930257320404053\n",
      "        vf_explained_var: 0.34083884954452515\n",
      "        vf_loss: 2.3383915424346924\n",
      "    load_time_ms: 1234.842\n",
      "    num_steps_sampled: 36864\n",
      "    num_steps_trained: 36864\n",
      "    sample_time_ms: 38296.61\n",
      "    update_time_ms: 6177.048\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.0.251.116\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.51904761904763\n",
      "    ram_util_percent: 44.10476190476192\n",
      "  pid: 111\n",
      "  policy_reward_max:\n",
      "    policy_0: 15.0\n",
      "    policy_1: 18.0\n",
      "    policy_2: 16.0\n",
      "    policy_3: 18.0\n",
      "    policy_4: 15.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 2.747711670480549\n",
      "    policy_1: 2.977688787185355\n",
      "    policy_2: 2.9302059496567505\n",
      "    policy_3: 2.679633867276888\n",
      "    policy_4: 2.696796338672769\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.205197177216063\n",
      "    mean_inference_ms: 7.1890190132573295\n",
      "    mean_processing_ms: 2.6384899936425463\n",
      "  time_since_restore: 239.58106589317322\n",
      "  time_this_iter_s: 48.99640893936157\n",
      "  time_total_s: 239.58106589317322\n",
      "  timestamp: 1646252121\n",
      "  timesteps_since_restore: 36864\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 36864\n",
      "  training_iteration: 4\n",
      "  trial_id: d1764e86\n",
      "  worst_snake_episode_len_max: 15\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_d1764e86 | RUNNING  | 10.0.251.116:111 |   14.032 |          239.581 | 36864 |      4 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_d1764e86:\n",
      "  best_snake_episode_len_max: 19\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 10\n",
      "    Forbidden_move_mean: 4.1046979865771815\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 3\n",
      "    Killed_another_snake_mean: 0.18389261744966443\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 5\n",
      "    Snake_hit_body_mean: 0.3919463087248322\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 4\n",
      "    Snake_hit_wall_mean: 0.5322147651006711\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.46375838926174495\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 15\n",
      "    policy0_max_len_mean: 3.1986577181208053\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 19\n",
      "    policy1_max_len_mean: 3.634228187919463\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 18\n",
      "    policy2_max_len_mean: 3.4805369127516776\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 18\n",
      "    policy3_max_len_mean: 3.2013422818791946\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 16\n",
      "    policy4_max_len_mean: 3.3140939597315437\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-03-02_20-16-10\n",
      "  done: false\n",
      "  episode_len_max: 19\n",
      "  episode_len_mean: 6.188590604026846\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 50.0\n",
      "  episode_reward_mean: 16.828859060402685\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 1490\n",
      "  episodes_total: 9376\n",
      "  experiment_id: d48644d2bc65443c8bfbbc4cf16746cc\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-251-116.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 13382.743\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2149155139923096\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017843708395957947\n",
      "        policy_loss: -0.05717037618160248\n",
      "        total_loss: 2.5697693824768066\n",
      "        vf_explained_var: 0.3827957510948181\n",
      "        vf_loss: 2.618910312652588\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1195420026779175\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020229347050189972\n",
      "        policy_loss: -0.05206605792045593\n",
      "        total_loss: 3.3220620155334473\n",
      "        vf_explained_var: 0.37172991037368774\n",
      "        vf_loss: 3.3650248050689697\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1414086818695068\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02094915881752968\n",
      "        policy_loss: -0.05433633178472519\n",
      "        total_loss: 3.2050106525421143\n",
      "        vf_explained_var: 0.36264434456825256\n",
      "        vf_loss: 3.2499196529388428\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.166077733039856\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021681861951947212\n",
      "        policy_loss: -0.055588237941265106\n",
      "        total_loss: 2.803868293762207\n",
      "        vf_explained_var: 0.38735735416412354\n",
      "        vf_loss: 2.8496997356414795\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2077736854553223\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01906978152692318\n",
      "        policy_loss: -0.054771628230810165\n",
      "        total_loss: 2.811307430267334\n",
      "        vf_explained_var: 0.37328022718429565\n",
      "        vf_loss: 2.857497453689575\n",
      "    load_time_ms: 1123.085\n",
      "    num_steps_sampled: 46080\n",
      "    num_steps_trained: 46080\n",
      "    sample_time_ms: 37837.69\n",
      "    update_time_ms: 4946.202\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.0.251.116\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.7953125\n",
      "    ram_util_percent: 44.26875\n",
      "  pid: 111\n",
      "  policy_reward_max:\n",
      "    policy_0: 15.0\n",
      "    policy_1: 19.0\n",
      "    policy_2: 18.0\n",
      "    policy_3: 18.0\n",
      "    policy_4: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 3.1986577181208053\n",
      "    policy_1: 3.634228187919463\n",
      "    policy_2: 3.4805369127516776\n",
      "    policy_3: 3.2013422818791946\n",
      "    policy_4: 3.3140939597315437\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.2066800433866316\n",
      "    mean_inference_ms: 7.139235849136353\n",
      "    mean_processing_ms: 2.565785167331932\n",
      "  time_since_restore: 288.6017236709595\n",
      "  time_this_iter_s: 49.020657777786255\n",
      "  time_total_s: 288.6017236709595\n",
      "  timestamp: 1646252170\n",
      "  timesteps_since_restore: 46080\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 46080\n",
      "  training_iteration: 5\n",
      "  trial_id: d1764e86\n",
      "  worst_snake_episode_len_max: 15\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_d1764e86 | RUNNING  | 10.0.251.116:111 |  16.8289 |          288.602 | 46080 |      5 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_d1764e86:\n",
      "  best_snake_episode_len_max: 37\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 8\n",
      "    Forbidden_move_mean: 4.031660231660232\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 3\n",
      "    Killed_another_snake_mean: 0.25096525096525096\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 4\n",
      "    Snake_hit_body_mean: 0.47876447876447875\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 4\n",
      "    Snake_hit_wall_mean: 0.37065637065637064\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.5691119691119692\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 20\n",
      "    policy0_max_len_mean: 3.7166023166023168\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 37\n",
      "    policy1_max_len_mean: 4.197683397683398\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 36\n",
      "    policy2_max_len_mean: 4.193050193050193\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 17\n",
      "    policy3_max_len_mean: 3.881853281853282\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 18\n",
      "    policy4_max_len_mean: 3.6362934362934363\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-03-02_20-16-58\n",
      "  done: false\n",
      "  episode_len_max: 37\n",
      "  episode_len_mean: 7.10965250965251\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 80.0\n",
      "  episode_reward_mean: 19.625482625482626\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 1295\n",
      "  episodes_total: 10671\n",
      "  experiment_id: d48644d2bc65443c8bfbbc4cf16746cc\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-251-116.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 13073.758\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1496864557266235\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020293742418289185\n",
      "        policy_loss: -0.05186835676431656\n",
      "        total_loss: 3.0918266773223877\n",
      "        vf_explained_var: 0.4052175283432007\n",
      "        vf_loss: 3.1345629692077637\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0752562284469604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014681398868560791\n",
      "        policy_loss: -0.04990005865693092\n",
      "        total_loss: 3.8422610759735107\n",
      "        vf_explained_var: 0.4115651845932007\n",
      "        vf_loss: 3.8822500705718994\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.079952359199524\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014947358518838882\n",
      "        policy_loss: -0.04524509608745575\n",
      "        total_loss: 3.8038644790649414\n",
      "        vf_explained_var: 0.3994542062282562\n",
      "        vf_loss: 3.839020252227783\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0835171937942505\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018146760761737823\n",
      "        policy_loss: -0.05377412959933281\n",
      "        total_loss: 3.170661211013794\n",
      "        vf_explained_var: 0.4454236626625061\n",
      "        vf_loss: 3.212186336517334\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1497210264205933\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021488141268491745\n",
      "        policy_loss: -0.053359754383563995\n",
      "        total_loss: 3.052661657333374\n",
      "        vf_explained_var: 0.4107426404953003\n",
      "        vf_loss: 3.0963516235351562\n",
      "    load_time_ms: 1057.354\n",
      "    num_steps_sampled: 55296\n",
      "    num_steps_trained: 55296\n",
      "    sample_time_ms: 37470.117\n",
      "    update_time_ms: 4125.353\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.0.251.116\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.78709677419354\n",
      "    ram_util_percent: 44.26935483870968\n",
      "  pid: 111\n",
      "  policy_reward_max:\n",
      "    policy_0: 20.0\n",
      "    policy_1: 37.0\n",
      "    policy_2: 36.0\n",
      "    policy_3: 17.0\n",
      "    policy_4: 18.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 3.7166023166023168\n",
      "    policy_1: 4.197683397683398\n",
      "    policy_2: 4.193050193050193\n",
      "    policy_3: 3.881853281853282\n",
      "    policy_4: 3.6362934362934363\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.209963808509853\n",
      "    mean_inference_ms: 7.096302816148719\n",
      "    mean_processing_ms: 2.4968463481947842\n",
      "  time_since_restore: 336.5735261440277\n",
      "  time_this_iter_s: 47.97180247306824\n",
      "  time_total_s: 336.5735261440277\n",
      "  timestamp: 1646252218\n",
      "  timesteps_since_restore: 55296\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 55296\n",
      "  training_iteration: 6\n",
      "  trial_id: d1764e86\n",
      "  worst_snake_episode_len_max: 17\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_d1764e86 | RUNNING  | 10.0.251.116:111 |  19.6255 |          336.574 | 55296 |      6 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_d1764e86:\n",
      "  best_snake_episode_len_max: 21\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 9\n",
      "    Forbidden_move_mean: 3.9403747870528107\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 4\n",
      "    Killed_another_snake_mean: 0.29557069846678025\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 6\n",
      "    Snake_hit_body_mean: 0.6132879045996593\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 3\n",
      "    Snake_hit_wall_mean: 0.26916524701873934\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.6252129471890971\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 19\n",
      "    policy0_max_len_mean: 4.165247018739352\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 20\n",
      "    policy1_max_len_mean: 4.7001703577512775\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 19\n",
      "    policy2_max_len_mean: 4.724020442930153\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 19\n",
      "    policy3_max_len_mean: 4.2751277683134585\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 21\n",
      "    policy4_max_len_mean: 4.237649063032368\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-03-02_20-17-47\n",
      "  done: false\n",
      "  episode_len_max: 21\n",
      "  episode_len_mean: 7.857751277683135\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 68.0\n",
      "  episode_reward_mean: 22.10221465076661\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 1174\n",
      "  episodes_total: 11845\n",
      "  experiment_id: d48644d2bc65443c8bfbbc4cf16746cc\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-251-116.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 12841.531\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.108352541923523\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014381430111825466\n",
      "        policy_loss: -0.0515265017747879\n",
      "        total_loss: 3.771923065185547\n",
      "        vf_explained_var: 0.42680543661117554\n",
      "        vf_loss: 3.8137423992156982\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.024722933769226\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01708707958459854\n",
      "        policy_loss: -0.04709858074784279\n",
      "        total_loss: 4.487315654754639\n",
      "        vf_explained_var: 0.4306164085865021\n",
      "        vf_loss: 4.522880554199219\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0085148811340332\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016270924359560013\n",
      "        policy_loss: -0.04505882412195206\n",
      "        total_loss: 4.3039164543151855\n",
      "        vf_explained_var: 0.43169236183166504\n",
      "        vf_loss: 4.3379926681518555\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0201209783554077\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016423432156443596\n",
      "        policy_loss: -0.04526986926794052\n",
      "        total_loss: 3.407914638519287\n",
      "        vf_explained_var: 0.48839569091796875\n",
      "        vf_loss: 3.442098617553711\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0967192649841309\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017104851081967354\n",
      "        policy_loss: -0.05324712023139\n",
      "        total_loss: 3.9232499599456787\n",
      "        vf_explained_var: 0.41618284583091736\n",
      "        vf_loss: 3.964951515197754\n",
      "    load_time_ms: 1003.668\n",
      "    num_steps_sampled: 64512\n",
      "    num_steps_trained: 64512\n",
      "    sample_time_ms: 37319.965\n",
      "    update_time_ms: 3539.001\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 10.0.251.116\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.62857142857143\n",
      "    ram_util_percent: 44.25555555555557\n",
      "  pid: 111\n",
      "  policy_reward_max:\n",
      "    policy_0: 19.0\n",
      "    policy_1: 20.0\n",
      "    policy_2: 19.0\n",
      "    policy_3: 19.0\n",
      "    policy_4: 21.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 4.165247018739352\n",
      "    policy_1: 4.7001703577512775\n",
      "    policy_2: 4.724020442930153\n",
      "    policy_3: 4.2751277683134585\n",
      "    policy_4: 4.237649063032368\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.2214873992769526\n",
      "    mean_inference_ms: 7.0993010596755335\n",
      "    mean_processing_ms: 2.443632440829655\n",
      "  time_since_restore: 385.20148825645447\n",
      "  time_this_iter_s: 48.62796211242676\n",
      "  time_total_s: 385.20148825645447\n",
      "  timestamp: 1646252267\n",
      "  timesteps_since_restore: 64512\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 64512\n",
      "  training_iteration: 7\n",
      "  trial_id: d1764e86\n",
      "  worst_snake_episode_len_max: 19\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_d1764e86 | RUNNING  | 10.0.251.116:111 |  22.1022 |          385.201 | 64512 |      7 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_d1764e86:\n",
      "  best_snake_episode_len_max: 26\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 9\n",
      "    Forbidden_move_mean: 3.8449905482041586\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 3\n",
      "    Killed_another_snake_mean: 0.3393194706994329\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 5\n",
      "    Snake_hit_body_mean: 0.6635160680529301\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 3\n",
      "    Snake_hit_wall_mean: 0.18714555765595464\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.6427221172022685\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 25\n",
      "    policy0_max_len_mean: 4.6351606805293\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 26\n",
      "    policy1_max_len_mean: 5.131379962192817\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 22\n",
      "    policy2_max_len_mean: 4.934782608695652\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 25\n",
      "    policy3_max_len_mean: 5.159735349716446\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 21\n",
      "    policy4_max_len_mean: 4.751417769376181\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-03-02_20-18-36\n",
      "  done: false\n",
      "  episode_len_max: 26\n",
      "  episode_len_mean: 8.693761814744802\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 80.0\n",
      "  episode_reward_mean: 24.612476370510397\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 1058\n",
      "  episodes_total: 12903\n",
      "  experiment_id: d48644d2bc65443c8bfbbc4cf16746cc\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-251-116.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 12677.325\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.062827229499817\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016494132578372955\n",
      "        policy_loss: -0.04440653696656227\n",
      "        total_loss: 4.495782375335693\n",
      "        vf_explained_var: 0.46526390314102173\n",
      "        vf_loss: 4.529055118560791\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9875603914260864\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014751751907169819\n",
      "        policy_loss: -0.048856787383556366\n",
      "        total_loss: 5.019139289855957\n",
      "        vf_explained_var: 0.45737946033477783\n",
      "        vf_loss: 5.05803918838501\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.994488000869751\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013499833643436432\n",
      "        policy_loss: -0.04185575991868973\n",
      "        total_loss: 4.43044376373291\n",
      "        vf_explained_var: 0.47054046392440796\n",
      "        vf_loss: 4.463186740875244\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.963475227355957\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01445145532488823\n",
      "        policy_loss: -0.04692218825221062\n",
      "        total_loss: 4.634792804718018\n",
      "        vf_explained_var: 0.4743664562702179\n",
      "        vf_loss: 4.67195987701416\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0420830249786377\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016702331602573395\n",
      "        policy_loss: -0.0491284504532814\n",
      "        total_loss: 4.497199535369873\n",
      "        vf_explained_var: 0.4415053129196167\n",
      "        vf_loss: 4.5350542068481445\n",
      "    load_time_ms: 970.733\n",
      "    num_steps_sampled: 73728\n",
      "    num_steps_trained: 73728\n",
      "    sample_time_ms: 37233.307\n",
      "    update_time_ms: 3099.527\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 10.0.251.116\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.0904761904762\n",
      "    ram_util_percent: 44.328571428571436\n",
      "  pid: 111\n",
      "  policy_reward_max:\n",
      "    policy_0: 25.0\n",
      "    policy_1: 26.0\n",
      "    policy_2: 22.0\n",
      "    policy_3: 25.0\n",
      "    policy_4: 21.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 4.6351606805293\n",
      "    policy_1: 5.131379962192817\n",
      "    policy_2: 4.934782608695652\n",
      "    policy_3: 5.159735349716446\n",
      "    policy_4: 4.751417769376181\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.237771364862262\n",
      "    mean_inference_ms: 7.102740151911271\n",
      "    mean_processing_ms: 2.3979633552068607\n",
      "  time_since_restore: 434.1793761253357\n",
      "  time_this_iter_s: 48.977887868881226\n",
      "  time_total_s: 434.1793761253357\n",
      "  timestamp: 1646252316\n",
      "  timesteps_since_restore: 73728\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 73728\n",
      "  training_iteration: 8\n",
      "  trial_id: d1764e86\n",
      "  worst_snake_episode_len_max: 21\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_d1764e86 | RUNNING  | 10.0.251.116:111 |  24.6125 |          434.179 | 73728 |      8 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_d1764e86:\n",
      "  best_snake_episode_len_max: 26\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 7\n",
      "    Forbidden_move_mean: 3.5868200836820083\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 4\n",
      "    Killed_another_snake_mean: 0.4110878661087866\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 6\n",
      "    Snake_hit_body_mean: 0.7709205020920502\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 4\n",
      "    Snake_hit_wall_mean: 0.17782426778242677\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 6\n",
      "    Snake_was_eaten_mean: 0.8158995815899581\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 25\n",
      "    policy0_max_len_mean: 5.158995815899582\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 25\n",
      "    policy1_max_len_mean: 5.911087866108787\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 26\n",
      "    policy2_max_len_mean: 5.649581589958159\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 24\n",
      "    policy3_max_len_mean: 5.46652719665272\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 23\n",
      "    policy4_max_len_mean: 5.194560669456067\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-03-02_20-19-24\n",
      "  done: false\n",
      "  episode_len_max: 26\n",
      "  episode_len_mean: 9.639121338912133\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 78.0\n",
      "  episode_reward_mean: 27.380753138075313\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 956\n",
      "  episodes_total: 13859\n",
      "  experiment_id: d48644d2bc65443c8bfbbc4cf16746cc\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-251-116.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 12546.502\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0356703996658325\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015098366886377335\n",
      "        policy_loss: -0.04320878908038139\n",
      "        total_loss: 5.0786285400390625\n",
      "        vf_explained_var: 0.49368613958358765\n",
      "        vf_loss: 5.1116461753845215\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9306230545043945\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015572956763207912\n",
      "        policy_loss: -0.04634202644228935\n",
      "        total_loss: 6.325346946716309\n",
      "        vf_explained_var: 0.478584349155426\n",
      "        vf_loss: 6.36117696762085\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9499927163124084\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01714407652616501\n",
      "        policy_loss: -0.03764248266816139\n",
      "        total_loss: 5.476035118103027\n",
      "        vf_explained_var: 0.47334086894989014\n",
      "        vf_loss: 5.502105712890625\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9134873151779175\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012664512731134892\n",
      "        policy_loss: -0.0434294193983078\n",
      "        total_loss: 4.6613264083862305\n",
      "        vf_explained_var: 0.5175368785858154\n",
      "        vf_loss: 4.696207523345947\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9835848212242126\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015083572827279568\n",
      "        policy_loss: -0.04656904190778732\n",
      "        total_loss: 5.129789352416992\n",
      "        vf_explained_var: 0.4813902676105499\n",
      "        vf_loss: 5.166176795959473\n",
      "    load_time_ms: 939.405\n",
      "    num_steps_sampled: 82944\n",
      "    num_steps_trained: 82944\n",
      "    sample_time_ms: 37102.14\n",
      "    update_time_ms: 2757.644\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 10.0.251.116\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.56129032258065\n",
      "    ram_util_percent: 44.44354838709677\n",
      "  pid: 111\n",
      "  policy_reward_max:\n",
      "    policy_0: 25.0\n",
      "    policy_1: 25.0\n",
      "    policy_2: 26.0\n",
      "    policy_3: 24.0\n",
      "    policy_4: 23.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 5.158995815899582\n",
      "    policy_1: 5.911087866108787\n",
      "    policy_2: 5.649581589958159\n",
      "    policy_3: 5.46652719665272\n",
      "    policy_4: 5.194560669456067\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.245817573746626\n",
      "    mean_inference_ms: 7.104607167685542\n",
      "    mean_processing_ms: 2.3509987379849178\n",
      "  time_since_restore: 482.4936761856079\n",
      "  time_this_iter_s: 48.31430006027222\n",
      "  time_total_s: 482.4936761856079\n",
      "  timestamp: 1646252364\n",
      "  timesteps_since_restore: 82944\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 82944\n",
      "  training_iteration: 9\n",
      "  trial_id: d1764e86\n",
      "  worst_snake_episode_len_max: 23\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.6/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_d1764e86 | RUNNING  | 10.0.251.116:111 |  27.3808 |          482.494 | 82944 |      9 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_d1764e86:\n",
      "  best_snake_episode_len_max: 32\n",
      "  custom_metrics:\n",
      "    Forbidden_move_max: 8\n",
      "    Forbidden_move_mean: 3.4829931972789114\n",
      "    Forbidden_move_min: 0\n",
      "    Killed_another_snake_max: 4\n",
      "    Killed_another_snake_mean: 0.4557823129251701\n",
      "    Killed_another_snake_min: 0\n",
      "    Snake_hit_body_max: 4\n",
      "    Snake_hit_body_mean: 0.8469387755102041\n",
      "    Snake_hit_body_min: 0\n",
      "    Snake_hit_wall_max: 2\n",
      "    Snake_hit_wall_mean: 0.14058956916099774\n",
      "    Snake_hit_wall_min: 0\n",
      "    Snake_was_eaten_max: 8\n",
      "    Snake_was_eaten_mean: 0.8458049886621315\n",
      "    Snake_was_eaten_min: 0\n",
      "    Starved_max: 0\n",
      "    Starved_mean: 0.0\n",
      "    Starved_min: 0\n",
      "    policy0_max_len_max: 28\n",
      "    policy0_max_len_mean: 5.4229024943310655\n",
      "    policy0_max_len_min: 0\n",
      "    policy1_max_len_max: 31\n",
      "    policy1_max_len_mean: 6.456916099773243\n",
      "    policy1_max_len_min: 0\n",
      "    policy2_max_len_max: 30\n",
      "    policy2_max_len_mean: 6.187074829931973\n",
      "    policy2_max_len_min: 0\n",
      "    policy3_max_len_max: 28\n",
      "    policy3_max_len_mean: 5.976190476190476\n",
      "    policy3_max_len_min: 0\n",
      "    policy4_max_len_max: 32\n",
      "    policy4_max_len_mean: 5.741496598639456\n",
      "    policy4_max_len_min: 0\n",
      "  date: 2022-03-02_20-20-12\n",
      "  done: true\n",
      "  episode_len_max: 32\n",
      "  episode_len_mean: 10.477324263038549\n",
      "  episode_len_min: 1\n",
      "  episode_reward_max: 82.0\n",
      "  episode_reward_mean: 29.784580498866212\n",
      "  episode_reward_min: 1.0\n",
      "  episodes_this_iter: 882\n",
      "  episodes_total: 14741\n",
      "  experiment_id: d48644d2bc65443c8bfbbc4cf16746cc\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-251-116.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 12517.687\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9870590567588806\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014017697423696518\n",
      "        policy_loss: -0.04850446432828903\n",
      "        total_loss: 5.136322021484375\n",
      "        vf_explained_var: 0.506837010383606\n",
      "        vf_loss: 5.175364971160889\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.883155345916748\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015439380891621113\n",
      "        policy_loss: -0.03937738388776779\n",
      "        total_loss: 6.733044624328613\n",
      "        vf_explained_var: 0.4904422163963318\n",
      "        vf_loss: 6.762001037597656\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9056652784347534\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01349275279790163\n",
      "        policy_loss: -0.03991755470633507\n",
      "        total_loss: 6.927705764770508\n",
      "        vf_explained_var: 0.4788767993450165\n",
      "        vf_loss: 6.9585161209106445\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8931779861450195\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01741934008896351\n",
      "        policy_loss: -0.03985222801566124\n",
      "        total_loss: 4.870591640472412\n",
      "        vf_explained_var: 0.5478007197380066\n",
      "        vf_loss: 4.898685932159424\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9286647439002991\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015195177868008614\n",
      "        policy_loss: -0.04232257232069969\n",
      "        total_loss: 6.042605876922607\n",
      "        vf_explained_var: 0.5008127093315125\n",
      "        vf_loss: 6.074671745300293\n",
      "    load_time_ms: 914.026\n",
      "    num_steps_sampled: 92160\n",
      "    num_steps_trained: 92160\n",
      "    sample_time_ms: 36858.187\n",
      "    update_time_ms: 2483.828\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 10.0.251.116\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.05\n",
      "    ram_util_percent: 44.64193548387097\n",
      "  pid: 111\n",
      "  policy_reward_max:\n",
      "    policy_0: 28.0\n",
      "    policy_1: 31.0\n",
      "    policy_2: 30.0\n",
      "    policy_3: 28.0\n",
      "    policy_4: 32.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: 5.4229024943310655\n",
      "    policy_1: 6.456916099773243\n",
      "    policy_2: 6.187074829931973\n",
      "    policy_3: 5.976190476190476\n",
      "    policy_4: 5.741496598639456\n",
      "  policy_reward_min:\n",
      "    policy_0: 0.0\n",
      "    policy_1: 0.0\n",
      "    policy_2: 0.0\n",
      "    policy_3: 0.0\n",
      "    policy_4: 0.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.245086062513494\n",
      "    mean_inference_ms: 7.077989295134928\n",
      "    mean_processing_ms: 2.30184944678552\n",
      "  time_since_restore: 530.1671938896179\n",
      "  time_this_iter_s: 47.67351770401001\n",
      "  time_total_s: 530.1671938896179\n",
      "  timestamp: 1646252412\n",
      "  timesteps_since_restore: 92160\n",
      "  timesteps_this_iter: 9216\n",
      "  timesteps_total: 92160\n",
      "  training_iteration: 10\n",
      "  trial_id: d1764e86\n",
      "  worst_snake_episode_len_max: 28\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.6/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_d1764e86 | RUNNING  | 10.0.251.116:111 |  29.7846 |          530.167 | 92160 |     10 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.6/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 0/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 TERMINATED)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+------------+-------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status     | loc   |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+------------+-------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_d1764e86 | TERMINATED |       |  29.7846 |          530.167 | 92160 |     10 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+------------+-------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34mSaved model configuration.\u001b[0m\n",
      "\u001b[34mSaved the checkpoint file /opt/ml/output/intermediate/training/PPO_MultiAgentBattlesnake-v1_d1764e86_0_2022-03-02_20-10-3460izj72s/checkpoint_10/checkpoint-10 as /opt/ml/model/checkpoint\u001b[0m\n",
      "\u001b[34mSaved the checkpoint file /opt/ml/output/intermediate/training/PPO_MultiAgentBattlesnake-v1_d1764e86_0_2022-03-02_20-10-3460izj72s/checkpoint_10/checkpoint-10.tune_metadata as /opt/ml/model/checkpoint.tune_metadata\u001b[0m\n",
      "\u001b[34m2022-03-02 20:20:17,663#011INFO trainer.py:420 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\u001b[0m\n",
      "\u001b[34m2022-03-02 20:20:17,675#011INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#033[2m#033[36m(pid=1049)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1049)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1049)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1049)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1049)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1049)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1049)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1049)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1049)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=1049)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m2022-03-02 20:20:50,048#011INFO trainable.py:178 -- _setup took 32.376 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\u001b[0m\n",
      "\u001b[34m2022-03-02 20:20:51,965#011WARNING trainable.py:210 -- Getting current IP.\u001b[0m\n",
      "\u001b[34m2022-03-02 20:20:51,965#011INFO trainable.py:416 -- Restored on 10.0.251.116 from checkpoint: /opt/ml/model/checkpoint\u001b[0m\n",
      "\u001b[34m2022-03-02 20:20:51,965#011INFO trainable.py:423 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': 92160, '_time_total': 530.1671938896179, '_episodes_total': 14741}\u001b[0m\n",
      "\u001b[34mSaved TensorFlow serving model!\u001b[0m\n",
      "\u001b[34m2022-03-02 20:20:57,518 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-03-02 20:21:11 Uploading - Uploading generated training model\n",
      "2022-03-02 20:21:11 Completed - Training job completed\n",
      "Training seconds: 720\n",
      "Billable seconds: 720\n",
      "Training job: sagemaker-soln-bs-job-rllib-2022-03-02-20-06-52-405\n",
      "CPU times: user 1.73 s, sys: 81.8 ms, total: 1.81 s\n",
      "Wall time: 14min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define and execute our training job\n",
    "# Adjust hyperparameters and train_instance_count accordingly\n",
    "\n",
    "metric_definitions =  [\n",
    "    {'Name': 'training_iteration', 'Regex': 'training_iteration: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episodes_total', 'Regex': 'episodes_total: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'num_steps_trained', 'Regex': 'num_steps_trained: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'timesteps_total', 'Regex': 'timesteps_total: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'training_iteration', 'Regex': 'training_iteration: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "\n",
    "    {'Name': 'episode_reward_max', 'Regex': 'episode_reward_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episode_reward_mean', 'Regex': 'episode_reward_mean: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episode_reward_min', 'Regex': 'episode_reward_min: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    \n",
    "    {'Name': 'episode_len_max', 'Regex': 'episode_len_mean: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episode_len_mean', 'Regex': 'episode_len_mean: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episode_len_min', 'Regex': 'episode_len_mean: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "\n",
    "    {'Name': 'best_snake_episode_len_max', 'Regex': 'best_snake_episode_len_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'worst_snake_episode_len_max', 'Regex': 'worst_snake_episode_len_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "\n",
    "    {'Name': 'Snake_hit_wall_max', 'Regex': 'Snake_hit_wall_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'Snake_was_eaten_max', 'Regex': 'Snake_was_eaten_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'Killed_another_snake_max', 'Regex': 'Killed_another_snake_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'Snake_hit_body_max', 'Regex': 'Snake_hit_body_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'Starved_max', 'Regex': 'Starved_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'Forbidden_move_max', 'Regex': 'Forbidden_move_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}\n",
    "] \n",
    "\n",
    "algorithm = \"PPO\"\n",
    "map_size = 11\n",
    "num_agents = 5\n",
    "additional_config = {\n",
    "    'lambda': 0.90,\n",
    "    'gamma': 0.999,\n",
    "    'kl_coeff': 0.2,\n",
    "    'clip_rewards': True,\n",
    "    'vf_clip_param': 175.0,\n",
    "    'train_batch_size': 9216,\n",
    "    'sample_batch_size': 96,\n",
    "    'sgd_minibatch_size': 256,\n",
    "    'num_sgd_iter': 3,\n",
    "    'lr': 5.0e-4,\n",
    "}\n",
    "\n",
    "estimator = RLEstimator(entry_point=\"train-mabs.py\",\n",
    "                        source_dir='training/training_src',\n",
    "                        dependencies=[\"training/common/sagemaker_rl\", \"inference/inference_src/\", \"../BattlesnakeGym/\"],\n",
    "                        image_uri=image_name,\n",
    "                        role=role,\n",
    "                        train_instance_type=instance_type,\n",
    "                        train_instance_count=1,\n",
    "                        output_path=s3_output_path,\n",
    "                        base_job_name=job_name_prefix,\n",
    "                        metric_definitions=metric_definitions,\n",
    "                        hyperparameters={\n",
    "                            # See train-mabs.py to add additional hyperparameters\n",
    "                            # Also see ray_launcher.py for the rl.training.* hyperparameters\n",
    "                            \n",
    "                            \"num_iters\": 10,\n",
    "                            # number of snakes in the gym\n",
    "                            \"num_agents\": num_agents,\n",
    "\n",
    "                            \"iterate_map_size\": False,\n",
    "                            \"map_size\": map_size,\n",
    "                            \"algorithm\": algorithm,\n",
    "                            \"additional_configs\": additional_config,\n",
    "                            \"use_heuristics_action_masks\": False\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "estimator.fit()\n",
    "\n",
    "job_name = estimator.latest_training_job.job_name\n",
    "print(\"Training job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-soln-bs-rbcsnake-bucket/sagemaker-soln-bs-job-rllib-2022-03-02-20-06-52-405/output/model.tar.gz'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Where is the model stored in S3?\n",
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an endpoint to host the policy\n",
    "Firstly, we will delete the previous endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 6.2 MiB/6.2 MiB (23.1 MiB/s) with 1 file(s) remaining\r",
      "copy: s3://sagemaker-soln-bs-rbcsnake-bucket/sagemaker-soln-bs-job-rllib-2022-03-02-20-06-52-405/output/model.tar.gz to s3://sagemaker-soln-bs-rbcsnake-bucket/pretrainedmodels/model.tar.gz\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.tensorflow.serving.Model has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=info['SagemakerEndPointName'])\n",
    "try:\n",
    "    sm_client.delete_endpoint(EndpointName=info['SagemakerEndPointName'])\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=info['SagemakerEndPointName'])\n",
    "    sm_client.delete_model(ModelName=info['SagemakerEndPointName'])\n",
    "    ep_waiter = sm_client.get_waiter('endpoint_deleted')\n",
    "    ep_waiter.wait(EndpointName=info['SagemakerEndPointName'])\n",
    "except botocore.exceptions.ClientError:\n",
    "    pass\n",
    "    \n",
    "# Copy the endpoint to a central location\n",
    "model_data = \"s3://{}/pretrainedmodels/model.tar.gz\".format(s3_bucket)\n",
    "!aws s3 cp {estimator.model_data} {model_data}\n",
    "\n",
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "model = Model(model_data=model_data,\n",
    "              role=role,\n",
    "              entry_point=\"inference.py\",\n",
    "              source_dir='inference/inference_src',\n",
    "              framework_version='2.1.0',\n",
    "              name=info['SagemakerEndPointName'],\n",
    "              code_location='s3://{}//code'.format(s3_bucket)\n",
    "             )\n",
    "\n",
    "if local_mode:\n",
    "    inf_instance_type = 'local'\n",
    "else:\n",
    "    inf_instance_type = info[\"SagemakerInferenceInstanceType\"]\n",
    "\n",
    "# Deploy an inference endpoint\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=inf_instance_type,\n",
    "                         endpoint_name=info['SagemakerEndPointName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the endpoint\n",
    "\n",
    "This example is using single observation for a 5-agent environment \n",
    "The last axis is 12 because the current MultiAgentEnv is concatenating 2 frames\n",
    "5 agent maps + 1 food map = 6 maps total    6 maps * 2 frames = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action to take 2\n",
      "Inference took 2757.06 ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "state = np.zeros(shape=(1, 21, 21, 6), dtype=np.float32).tolist()\n",
    "\n",
    "health_dict = {0: 50, 1: 50}\n",
    "json = {\"turn\": 4,\n",
    "        \"board\": {\n",
    "                \"height\": 11,\n",
    "                \"width\": 11,\n",
    "                \"food\": [],\n",
    "                \"snakes\": []\n",
    "                },\n",
    "            \"you\": {\n",
    "                \"id\": \"snake-id-string\",\n",
    "                \"name\": \"Sneky Snek\",\n",
    "                \"health\": 90,\n",
    "                \"body\": [{\"x\": 1, \"y\": 3}]\n",
    "                }\n",
    "            }\n",
    "\n",
    "before = time()\n",
    "action_mask = np.array([1, 1, 1, 1]).tolist()\n",
    "\n",
    "action = predictor.predict({\"state\": state, \"action_mask\": action_mask,\n",
    "                            \"prev_action\": -1, \n",
    "                           \"prev_reward\": -1, \"seq_lens\": -1,  \n",
    "                           \"all_health\": health_dict, \"json\": json})\n",
    "elapsed = time() - before\n",
    "\n",
    "action_to_take = action[\"outputs\"][\"heuristisc_action\"]\n",
    "print(\"Action to take {}\".format(action_to_take))\n",
    "print(\"Inference took %.2f ms\" % (elapsed*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "- To go back to the introduction click [here](./1_Introduction.ipynb)\n",
    "- To build some heuristics click [here](./3_HeuristicsDeveloper.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
